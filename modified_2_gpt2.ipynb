{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhJbnLGdxQ71X0eYENdMTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "501aab92d9854518b91d90e545bd22da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a72e247fbd9b4e85804eb35298e39196",
              "IPY_MODEL_503dcecf04c1402e9ca251d30fa0baf6",
              "IPY_MODEL_29d8169cfa434e73bf33b8b4afa5d002"
            ],
            "layout": "IPY_MODEL_a3859254c0f64bf8a0d60fe679150b44"
          }
        },
        "a72e247fbd9b4e85804eb35298e39196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4a19eb0023d4c09aeb084299aa8d113",
            "placeholder": "​",
            "style": "IPY_MODEL_ab99d7725cd043d19ba614f7ad041d28",
            "value": "config.json: 100%"
          }
        },
        "503dcecf04c1402e9ca251d30fa0baf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb5f922b8fac43a097b551f22173fe49",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfe2504c193e48d0bf109912f953391c",
            "value": 665
          }
        },
        "29d8169cfa434e73bf33b8b4afa5d002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58cb979b55c1416eb575fef2f7071993",
            "placeholder": "​",
            "style": "IPY_MODEL_4d809795b4ce4e639e43a2823f876eb0",
            "value": " 665/665 [00:00&lt;00:00, 51.4kB/s]"
          }
        },
        "a3859254c0f64bf8a0d60fe679150b44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4a19eb0023d4c09aeb084299aa8d113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab99d7725cd043d19ba614f7ad041d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb5f922b8fac43a097b551f22173fe49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfe2504c193e48d0bf109912f953391c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58cb979b55c1416eb575fef2f7071993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d809795b4ce4e639e43a2823f876eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a06a9df9dddc43bb9cae2edd3ceb07d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8603bd7cecab48f19f690b863989c0e8",
              "IPY_MODEL_89d7952eb4ec4d6d91fa5817c2e30bff",
              "IPY_MODEL_e715c946ec934aab91b4c1ddc1c184ea"
            ],
            "layout": "IPY_MODEL_9ef47759bf1543b8ad2022530f51dfca"
          }
        },
        "8603bd7cecab48f19f690b863989c0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab924ff7a7c443098e339102a38f4b3a",
            "placeholder": "​",
            "style": "IPY_MODEL_08df044499db4447bebd087f88792877",
            "value": "model.safetensors: 100%"
          }
        },
        "89d7952eb4ec4d6d91fa5817c2e30bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_990a592654cd4b27a78fa0c9d2ac2f21",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d8e518e33cb46b1ae5cec896ddd4e37",
            "value": 548105171
          }
        },
        "e715c946ec934aab91b4c1ddc1c184ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_244d7ac25cb34f8dad3d1040c74c8e15",
            "placeholder": "​",
            "style": "IPY_MODEL_77963fe50e4a4f5990933084562e1e33",
            "value": " 548M/548M [00:02&lt;00:00, 252MB/s]"
          }
        },
        "9ef47759bf1543b8ad2022530f51dfca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab924ff7a7c443098e339102a38f4b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08df044499db4447bebd087f88792877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "990a592654cd4b27a78fa0c9d2ac2f21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d8e518e33cb46b1ae5cec896ddd4e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "244d7ac25cb34f8dad3d1040c74c8e15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77963fe50e4a4f5990933084562e1e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0aa5f9dee04e49f1bd3d23a676ffc526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e2f344d5e354b649710ea3757fa34e7",
              "IPY_MODEL_d5902f6743aa45e4b7ade5828d72a730",
              "IPY_MODEL_f93fa08962034d50b3a461541246ef37"
            ],
            "layout": "IPY_MODEL_b6379966fb6d4601b875d8d455bde207"
          }
        },
        "9e2f344d5e354b649710ea3757fa34e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01d3909a50804db49b1b4fd38ef00be4",
            "placeholder": "​",
            "style": "IPY_MODEL_0a2d4c76fa0243939c00890faf6a30fc",
            "value": "generation_config.json: 100%"
          }
        },
        "d5902f6743aa45e4b7ade5828d72a730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20aaa8d239a046f1a57a816ecefcbe9b",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c88c16c634b4fb1a6622ce2a00cdeb7",
            "value": 124
          }
        },
        "f93fa08962034d50b3a461541246ef37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e935829e40c9433a85bb7b95819dc153",
            "placeholder": "​",
            "style": "IPY_MODEL_ce6a3e8b77334b308a2cbc141f9ceb8c",
            "value": " 124/124 [00:00&lt;00:00, 8.25kB/s]"
          }
        },
        "b6379966fb6d4601b875d8d455bde207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01d3909a50804db49b1b4fd38ef00be4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a2d4c76fa0243939c00890faf6a30fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20aaa8d239a046f1a57a816ecefcbe9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c88c16c634b4fb1a6622ce2a00cdeb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e935829e40c9433a85bb7b95819dc153": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce6a3e8b77334b308a2cbc141f9ceb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d69c5153b727402a8778dc9ee8bdb2be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_969e019ca9aa4277b0fa101aad48eed8",
              "IPY_MODEL_cb6821b60e9447febd30782c47c42d3b",
              "IPY_MODEL_5fff2c099783449788da541ac4248c70"
            ],
            "layout": "IPY_MODEL_bbb437e3fc9d4a9ea62d8e721614ae33"
          }
        },
        "969e019ca9aa4277b0fa101aad48eed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_facf33d518a84ba2bbaec714d256695f",
            "placeholder": "​",
            "style": "IPY_MODEL_4fcb2ea2a28f46c5a5c3df8480423946",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "cb6821b60e9447febd30782c47c42d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61a12921971044eca41155925601dd67",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd75dbda7ee6440c81a74d08ec46a3da",
            "value": 26
          }
        },
        "5fff2c099783449788da541ac4248c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08b8086093214864bd0102a630753be6",
            "placeholder": "​",
            "style": "IPY_MODEL_6cd4444ac9cd4c1784cb4e44fca5755f",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.83kB/s]"
          }
        },
        "bbb437e3fc9d4a9ea62d8e721614ae33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "facf33d518a84ba2bbaec714d256695f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fcb2ea2a28f46c5a5c3df8480423946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61a12921971044eca41155925601dd67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd75dbda7ee6440c81a74d08ec46a3da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08b8086093214864bd0102a630753be6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cd4444ac9cd4c1784cb4e44fca5755f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "289521e212274846b7090cc987a4b532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fa6ef85c9924323b153194b28ec1a3d",
              "IPY_MODEL_804ce020cb544f06964850503a382f20",
              "IPY_MODEL_d0308687e8a04d41af1a90bbdb8fa5e4"
            ],
            "layout": "IPY_MODEL_bddf19afc27c4e978c2885980e9da94f"
          }
        },
        "6fa6ef85c9924323b153194b28ec1a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10e0d9af798647a489a914adc3b57cb6",
            "placeholder": "​",
            "style": "IPY_MODEL_52125e5963274802b22a46d1d5186c45",
            "value": "vocab.json: 100%"
          }
        },
        "804ce020cb544f06964850503a382f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9186aef158c42aa9d6a9ae2c22710ef",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecece94338114780aa18f59f13d8b9d9",
            "value": 1042301
          }
        },
        "d0308687e8a04d41af1a90bbdb8fa5e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83b3db635b9a4a8699608526a1dc0907",
            "placeholder": "​",
            "style": "IPY_MODEL_82c88d1aed2b4e4fb52465b9ee10dac4",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 3.29MB/s]"
          }
        },
        "bddf19afc27c4e978c2885980e9da94f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10e0d9af798647a489a914adc3b57cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52125e5963274802b22a46d1d5186c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9186aef158c42aa9d6a9ae2c22710ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecece94338114780aa18f59f13d8b9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83b3db635b9a4a8699608526a1dc0907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82c88d1aed2b4e4fb52465b9ee10dac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "407185bbace640cda96354f0c47b95e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55854e8aaa484c938901ba61f39a3e11",
              "IPY_MODEL_7b2a67a796d4461fbe36f8fa74fd33e5",
              "IPY_MODEL_49e2989c338c47438bc568997aaa7524"
            ],
            "layout": "IPY_MODEL_40ea4a03c14f4513b61eb086db97dccd"
          }
        },
        "55854e8aaa484c938901ba61f39a3e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cf832e676934892b30a62b58d51618f",
            "placeholder": "​",
            "style": "IPY_MODEL_6e1b007c822a4835bd3f4035d7e36a92",
            "value": "merges.txt: 100%"
          }
        },
        "7b2a67a796d4461fbe36f8fa74fd33e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4291a453f88a411db15f07b87a8139ae",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79681163f52244b6b76bbc5859f2af59",
            "value": 456318
          }
        },
        "49e2989c338c47438bc568997aaa7524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8e6a7cd8004721b9881321692c6b7b",
            "placeholder": "​",
            "style": "IPY_MODEL_fabb336945b6459abbea0b31bcaed5ae",
            "value": " 456k/456k [00:01&lt;00:00, 405kB/s]"
          }
        },
        "40ea4a03c14f4513b61eb086db97dccd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cf832e676934892b30a62b58d51618f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e1b007c822a4835bd3f4035d7e36a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4291a453f88a411db15f07b87a8139ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79681163f52244b6b76bbc5859f2af59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a8e6a7cd8004721b9881321692c6b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fabb336945b6459abbea0b31bcaed5ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a92a0366886e4d4594c65b4637fd2ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13bf67cda8c54570a0c49fa06764717b",
              "IPY_MODEL_92533245f07045179e10255e16dd6a1b",
              "IPY_MODEL_45bf6152ee5d49e3b136a1b99f4d9d00"
            ],
            "layout": "IPY_MODEL_91e3bf7688e7406980a16c579195fc63"
          }
        },
        "13bf67cda8c54570a0c49fa06764717b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d1d5a31b66b4f0da8e70ac13669ebaa",
            "placeholder": "​",
            "style": "IPY_MODEL_50ee91a735ad42e2b1c6dc6280d6cf00",
            "value": "tokenizer.json: 100%"
          }
        },
        "92533245f07045179e10255e16dd6a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc3e4dbdaab74cb18c6da8b46b9d1161",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3635057b0f93432fbbe23d2f4fc9db4d",
            "value": 1355256
          }
        },
        "45bf6152ee5d49e3b136a1b99f4d9d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5f53b7e42674dcd9d5007ddcb76e1f1",
            "placeholder": "​",
            "style": "IPY_MODEL_1651fe5bf73c45a8a481db24ced2cde2",
            "value": " 1.36M/1.36M [00:09&lt;00:00, 136kB/s]"
          }
        },
        "91e3bf7688e7406980a16c579195fc63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d1d5a31b66b4f0da8e70ac13669ebaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ee91a735ad42e2b1c6dc6280d6cf00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc3e4dbdaab74cb18c6da8b46b9d1161": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3635057b0f93432fbbe23d2f4fc9db4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5f53b7e42674dcd9d5007ddcb76e1f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1651fe5bf73c45a8a481db24ced2cde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asad-Khan9/GPT-2-Flash-Attention-Data-Chunk-Optimization/blob/main/modified_2_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34xuMw_BwMqW",
        "outputId": "d1c7e061-a34a-4c44-9090-2c5d9ecf79c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, tiktoken, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 tiktoken-0.8.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tiktoken\n",
        "import datasets\n",
        "# URL for Alice's Adventures in Wonderland from Project Gutenberg\n",
        "url = 'https://www.gutenberg.org/files/11/11-0.txt'\n",
        "\n",
        "# Fetch the dataset (book)\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save it to input.txt\n",
        "full_text = response.text\n",
        "\n",
        "# Define the split ratio (80% train, 20% test)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(full_text) * split_ratio)\n",
        "\n",
        "# Split the dataset\n",
        "train_text = full_text[:split_index]\n",
        "test_text = full_text[split_index:]\n",
        "\n",
        "# Write to input.txt (full dataset)\n",
        "with open('input.txt', 'w') as f:\n",
        "    f.write(full_text)\n",
        "\n",
        "# Write to train.txt (80% of the dataset)\n",
        "with open('train.txt', 'w') as f:\n",
        "    f.write(train_text)\n",
        "\n",
        "# Write to test.txt (20% of the dataset)\n",
        "with open('test.txt', 'w') as f:\n",
        "    f.write(test_text)\n",
        "\n",
        "print(\"Dataset written to input.txt, train.txt (80%), and test.txt (20%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV-B8OxPwSHO",
        "outputId": "4b14ec96-87b7-4cd6-d9fd-9c2796a10940"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset written to input.txt, train.txt (80%), and test.txt (20%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "B8S3dpUpwSJr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash_attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70wMOlgsww3I",
        "outputId": "c18d36dc-d08c-472d-85ca-a5ee62a6d607"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash_attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/6.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash_attn) (2.5.1+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash_attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (2024.9.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->flash_attn)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->flash_attn)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->flash_attn)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash_attn)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash_attn)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash_attn)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->flash_attn)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash_attn)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash_attn)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash_attn)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash_attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash_attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash_attn) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash_attn\n",
            "  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash_attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187815463 sha256=d944fc7d2f962bce83fc4708c2fc0c21eaf8255962a0b350ae919362a51b7ef2\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "Successfully built flash_attn\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash_attn\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flash_attn-2.7.4.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "try:\n",
        "    from flash_attn import flash_attn_func\n",
        "    FLASH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FLASH_AVAILABLE = False\n",
        "    print(\"Flash attention not available. Using standard attention.\")\n",
        "\n",
        "class CausalSelfAttention(nn.Module):  #Improved attention module\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        # Configuration\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.use_flash = getattr(config, 'use_flash_attention', False) and FLASH_AVAILABLE\n",
        "        self.window_size = getattr(config, 'window_size', None)\n",
        "\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "        # Causal mask buffer\n",
        "        if not self.use_flash:\n",
        "            self.register_buffer(\n",
        "                \"bias\",\n",
        "                torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                .view(1, 1, config.block_size, config.block_size)\n",
        "            )\n",
        "\n",
        "    def _flash_attention(self, q, k, v):\n",
        "\n",
        "        # Reshape for flash attention\n",
        "        q = q.transpose(1, 2)  # [B, T, H, D]\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Apply flash attention\n",
        "        output = flash_attn_func(q, k, v, causal=True)\n",
        "        return output.transpose(1, 2)  # Return to original shape\n",
        "\n",
        "    def _sliding_window_attention(self, q, k, v, T):\n",
        "        # Sliding window attention implementation\n",
        "        att = torch.zeros_like(q @ k.transpose(-2, -1))\n",
        "        window_size = min(self.window_size, T) if self.window_size else T\n",
        "\n",
        "        for i in range(0, T, window_size):\n",
        "            end_idx = min(i + window_size, T)\n",
        "            # Calculate attention scores for the current window\n",
        "            scores = (q[:, :, i:end_idx] @ k[:, :, max(0, i-window_size):end_idx].transpose(-2, -1))\n",
        "            scores = scores * (1.0 / math.sqrt(k.size(-1)))\n",
        "\n",
        "            # Apply causal mask within the window\n",
        "            window_mask = self.bias[:, :, i:end_idx, max(0, i-window_size):end_idx]\n",
        "            scores = scores.masked_fill(window_mask == 0, float('-inf'))\n",
        "            scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "            # Update attention for the current window\n",
        "            att[:, :, i:end_idx] = scores @ v[:, :, max(0, i-window_size):end_idx]\n",
        "\n",
        "        return att\n",
        "\n",
        "    def _standard_attention(self, q, k, v, T):\n",
        "        # Regular attention implementation\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        return att @ v\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # Calculate query, key, values for all heads in batch\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        # Reshape for attention\n",
        "        head_size = C // self.n_head\n",
        "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "\n",
        "        # Apply attention based on configuration\n",
        "        if self.use_flash:\n",
        "            y = self._flash_attention(q, k, v)\n",
        "        elif self.window_size:\n",
        "            y = self._sliding_window_attention(q, k, v, T)\n",
        "        else:\n",
        "            y = self._standard_attention(q, k, v, T)\n",
        "\n",
        "        # Reshape and apply output projection\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.c_proj(y)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    use_flash_attention: bool = False  # Added config for flash attention\n",
        "    window_size: int = None  # Added config for sliding window attention\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and 'cuda' in device\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "X6J1q3MywSN2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    batch_size: int = 4  # Default batch size (B)\n",
        "    sequence_length: int = 1024  # Default sequence length (T)\n",
        "    chunk_size: int = 2048  # Size of chunks to process text\n",
        "    overlap: int = 128  # Overlap between chunks to maintain context\n",
        "    num_workers: int = 4  # Number of workers for DataLoader\n",
        "    cache_dir: str = \"cache\"  # Directory to store cached tokenized data\n",
        "\n",
        "class ImprovedDataset(Dataset):\n",
        "    def __init__(self, file_path: str, config: DataConfig, train_mode: bool = True):\n",
        "        \"\"\"\n",
        "        Improved dataset class with caching and chunking.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the input text file\n",
        "            config: Configuration for data processing\n",
        "            train_mode: If True, uses training data; if False, uses test data\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.train_mode = train_mode\n",
        "\n",
        "        # Create cache directory if it doesn't exist\n",
        "        Path(config.cache_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Determine cache file path\n",
        "        mode = \"train\" if train_mode else \"test\"\n",
        "        self.cache_file = Path(config.cache_dir) / f\"{mode}_cached.npy\"\n",
        "\n",
        "        # Load or create cached tokens\n",
        "        self.tokens = self._load_or_create_cache(file_path)\n",
        "\n",
        "        print(f\"Loaded {len(self.tokens)} tokens from {file_path}\")\n",
        "        print(f\"1 epoch = {len(self.tokens) // (config.batch_size * config.sequence_length)} batches\")\n",
        "\n",
        "        # Initialize position\n",
        "        self.current_position = 0\n",
        "\n",
        "    def _load_or_create_cache(self, file_path: str) -> torch.Tensor:\n",
        "        \"\"\"Load tokens from cache if available, otherwise create and cache them.\"\"\"\n",
        "        if self.cache_file.exists():\n",
        "            print(\"Loading from cache...\")\n",
        "            return torch.from_numpy(np.load(self.cache_file))\n",
        "\n",
        "        print(\"Cache not found. Processing text file...\")\n",
        "        return self._process_and_cache_file(file_path)\n",
        "\n",
        "    def _process_and_cache_file(self, file_path: str) -> torch.Tensor:\n",
        "        \"\"\"Process text file and create cache.\"\"\"\n",
        "        # Read text file\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Clean text\n",
        "        text = self._clean_text(text)\n",
        "\n",
        "        # Tokenize\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "\n",
        "        # Create chunks with overlap\n",
        "        chunks = []\n",
        "        for i in range(0, len(tokens), self.config.chunk_size - self.config.overlap):\n",
        "            chunk = tokens[i:i + self.config.chunk_size]\n",
        "            if len(chunk) >= self.config.sequence_length:\n",
        "                chunks.extend(chunk)\n",
        "\n",
        "        # Convert to numpy array and save cache\n",
        "        tokens_array = np.array(chunks, dtype=np.int64)\n",
        "        np.save(self.cache_file, tokens_array)\n",
        "\n",
        "        return torch.from_numpy(tokens_array)\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text.\"\"\"\n",
        "        # Remove multiple newlines\n",
        "        text = '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
        "        # Remove multiple spaces\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.config.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get sequence and target\n",
        "        seq_len = self.config.sequence_length\n",
        "        tokens = self.tokens[idx:idx + seq_len + 1]\n",
        "        x = tokens[:-1]\n",
        "        y = tokens[1:]\n",
        "        return x, y\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_file: str,\n",
        "    test_file: str,\n",
        "    config: DataConfig\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    \"\"\"Create train and test dataloaders.\"\"\"\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ImprovedDataset(train_file, config, train_mode=True)\n",
        "    test_dataset = ImprovedDataset(test_file, config, train_mode=False)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Initialize configuration\n",
        "config = DataConfig(\n",
        "    batch_size=4,  # Your original B\n",
        "    sequence_length=1024,  # Your original T\n",
        "    chunk_size=2048,\n",
        "    overlap=128,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# Create train and test dataloaders\n",
        "train_loader, test_loader = create_dataloaders(\n",
        "    train_file='train.txt',\n",
        "    test_file='test.txt',\n",
        "    config=config\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRvG6LAgwSQX",
        "outputId": "2a50b51f-2d59-43ef-e730-c0138250ea53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache not found. Processing text file...\n",
            "Loaded 34209 tokens from train.txt\n",
            "1 epoch = 8 batches\n",
            "Cache not found. Processing text file...\n",
            "Loaded 8192 tokens from test.txt\n",
            "1 epoch = 2 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFSxwSmhwSS0",
        "outputId": "a86ca301-8a83-480b-ad02-32b9c56871c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import time\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "# Initialize configuration and dataloaders\n",
        "config = DataConfig(\n",
        "    batch_size=4,  # Your original B\n",
        "    sequence_length=1024,  # Your original T\n",
        "    chunk_size=2048,\n",
        "    overlap=128,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# Create train and test dataloaders\n",
        "train_loader, test_loader = create_dataloaders(\n",
        "    train_file='train.txt',\n",
        "    test_file='test.txt',\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# Device configuration\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Initialize model\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "model = torch.compile(model)\n",
        "\n",
        "# Learning rate schedule configuration\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 50\n",
        "max_steps = 500\n",
        "\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
        "\n",
        "# Training loop\n",
        "step = 0\n",
        "for epoch in range((max_steps * config.batch_size) // len(train_loader) + 1):\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Move data to device\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update learning rate\n",
        "        lr = get_lr(step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Wait for GPU\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Calculate statistics\n",
        "        t1 = time.time()\n",
        "        dt = (t1 - t0) * 1000  # time difference in milliseconds\n",
        "        tokens_per_sec = (config.batch_size * config.sequence_length) / (t1 - t0)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"step {step:4d} | loss: {loss.item():.6f} | lr {lr:.4e} | \"\n",
        "              f\"norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "        step += 1\n",
        "        if step >= max_steps:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQfEEfvWwSVC",
        "outputId": "3c0370be-c1be-4ba3-9b41-bff03c6058ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from cache...\n",
            "Loaded 34209 tokens from train.txt\n",
            "1 epoch = 8 batches\n",
            "Loading from cache...\n",
            "Loaded 8192 tokens from test.txt\n",
            "1 epoch = 2 batches\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    0 | loss: 11.027405 | lr 1.2000e-05 | norm: 22.2122 | dt: 25260.97ms | tok/sec: 162.15\n",
            "step    1 | loss: 10.337845 | lr 2.4000e-05 | norm: 13.1779 | dt: 1476.04ms | tok/sec: 2774.99\n",
            "step    2 | loss: 9.722557 | lr 3.6000e-05 | norm: 8.8818 | dt: 1476.24ms | tok/sec: 2774.61\n",
            "step    3 | loss: 9.448235 | lr 4.8000e-05 | norm: 6.0001 | dt: 1477.49ms | tok/sec: 2772.26\n",
            "step    4 | loss: 9.083000 | lr 6.0000e-05 | norm: 5.8405 | dt: 1488.10ms | tok/sec: 2752.51\n",
            "step    5 | loss: 8.929291 | lr 7.2000e-05 | norm: 5.8820 | dt: 1484.85ms | tok/sec: 2758.52\n",
            "step    6 | loss: 8.496487 | lr 8.4000e-05 | norm: 9.2229 | dt: 1491.29ms | tok/sec: 2746.62\n",
            "step    7 | loss: 8.453780 | lr 9.6000e-05 | norm: 6.0118 | dt: 1499.22ms | tok/sec: 2732.08\n",
            "step    8 | loss: 8.393055 | lr 1.0800e-04 | norm: 7.6930 | dt: 1497.09ms | tok/sec: 2735.98\n",
            "step    9 | loss: 8.037750 | lr 1.2000e-04 | norm: 24.5127 | dt: 1497.70ms | tok/sec: 2734.86\n",
            "step   10 | loss: 7.908815 | lr 1.3200e-04 | norm: 4.2781 | dt: 1497.90ms | tok/sec: 2734.49\n",
            "step   11 | loss: 7.848441 | lr 1.4400e-04 | norm: 3.3454 | dt: 1509.76ms | tok/sec: 2713.01\n",
            "step   12 | loss: 7.968532 | lr 1.5600e-04 | norm: 3.1114 | dt: 1508.06ms | tok/sec: 2716.07\n",
            "step   13 | loss: 7.709227 | lr 1.6800e-04 | norm: 3.0724 | dt: 1512.93ms | tok/sec: 2707.32\n",
            "step   14 | loss: 7.719003 | lr 1.8000e-04 | norm: 4.6564 | dt: 1514.62ms | tok/sec: 2704.31\n",
            "step   15 | loss: 7.404949 | lr 1.9200e-04 | norm: 2.5435 | dt: 1519.80ms | tok/sec: 2695.09\n",
            "step   16 | loss: 7.097351 | lr 2.0400e-04 | norm: 2.2446 | dt: 1520.56ms | tok/sec: 2693.74\n",
            "step   17 | loss: 7.138934 | lr 2.1600e-04 | norm: 3.8126 | dt: 1526.68ms | tok/sec: 2682.95\n",
            "step   18 | loss: 6.989036 | lr 2.2800e-04 | norm: 1.9921 | dt: 1526.06ms | tok/sec: 2684.03\n",
            "step   19 | loss: 6.608828 | lr 2.4000e-04 | norm: 2.2480 | dt: 1531.54ms | tok/sec: 2674.43\n",
            "step   20 | loss: 6.441331 | lr 2.5200e-04 | norm: 2.2713 | dt: 1540.12ms | tok/sec: 2659.54\n",
            "step   21 | loss: 6.468199 | lr 2.6400e-04 | norm: 2.3632 | dt: 1537.87ms | tok/sec: 2663.42\n",
            "step   22 | loss: 5.912052 | lr 2.7600e-04 | norm: 3.0164 | dt: 1538.37ms | tok/sec: 2662.56\n",
            "step   23 | loss: 5.881364 | lr 2.8800e-04 | norm: 2.2172 | dt: 1547.42ms | tok/sec: 2646.98\n",
            "step   24 | loss: 5.803995 | lr 3.0000e-04 | norm: 2.0124 | dt: 1555.40ms | tok/sec: 2633.41\n",
            "step   25 | loss: 5.655549 | lr 3.1200e-04 | norm: 1.8720 | dt: 1561.61ms | tok/sec: 2622.94\n",
            "step   26 | loss: 5.723174 | lr 3.2400e-04 | norm: 2.3733 | dt: 1562.15ms | tok/sec: 2622.03\n",
            "step   27 | loss: 5.261079 | lr 3.3600e-04 | norm: 1.8465 | dt: 1558.46ms | tok/sec: 2628.23\n",
            "step   28 | loss: 5.222624 | lr 3.4800e-04 | norm: 2.6329 | dt: 1573.22ms | tok/sec: 2603.58\n",
            "step   29 | loss: 5.228714 | lr 3.6000e-04 | norm: 1.7653 | dt: 1576.09ms | tok/sec: 2598.84\n",
            "step   30 | loss: 5.150427 | lr 3.7200e-04 | norm: 1.3422 | dt: 1576.81ms | tok/sec: 2597.66\n",
            "step   31 | loss: 5.135645 | lr 3.8400e-04 | norm: 1.1153 | dt: 1576.64ms | tok/sec: 2597.93\n",
            "step   32 | loss: 5.039181 | lr 3.9600e-04 | norm: 1.5702 | dt: 1592.92ms | tok/sec: 2571.38\n",
            "step   33 | loss: 4.882133 | lr 4.0800e-04 | norm: 1.6953 | dt: 1598.02ms | tok/sec: 2563.18\n",
            "step   34 | loss: 4.895582 | lr 4.2000e-04 | norm: 1.4159 | dt: 1596.66ms | tok/sec: 2565.36\n",
            "step   35 | loss: 4.933684 | lr 4.3200e-04 | norm: 1.9219 | dt: 1597.25ms | tok/sec: 2564.41\n",
            "step   36 | loss: 4.839963 | lr 4.4400e-04 | norm: 1.5496 | dt: 1601.09ms | tok/sec: 2558.25\n",
            "step   37 | loss: 4.686057 | lr 4.5600e-04 | norm: 1.4934 | dt: 1610.00ms | tok/sec: 2544.10\n",
            "step   38 | loss: 4.998967 | lr 4.6800e-04 | norm: 1.6392 | dt: 1606.14ms | tok/sec: 2550.22\n",
            "step   39 | loss: 4.694540 | lr 4.8000e-04 | norm: 1.6260 | dt: 1612.10ms | tok/sec: 2540.79\n",
            "step   40 | loss: 4.871940 | lr 4.9200e-04 | norm: 1.4127 | dt: 1617.22ms | tok/sec: 2532.74\n",
            "step   41 | loss: 4.723675 | lr 5.0400e-04 | norm: 2.8376 | dt: 1618.91ms | tok/sec: 2530.09\n",
            "step   42 | loss: 4.749290 | lr 5.1600e-04 | norm: 1.8296 | dt: 1618.93ms | tok/sec: 2530.06\n",
            "step   43 | loss: 4.732337 | lr 5.2800e-04 | norm: 1.7963 | dt: 1634.80ms | tok/sec: 2505.50\n",
            "step   44 | loss: 4.742101 | lr 5.4000e-04 | norm: 1.6539 | dt: 1638.73ms | tok/sec: 2499.50\n",
            "step   45 | loss: 4.649652 | lr 5.5200e-04 | norm: 2.1438 | dt: 1644.81ms | tok/sec: 2490.26\n",
            "step   46 | loss: 4.309796 | lr 5.6400e-04 | norm: 3.8676 | dt: 1660.68ms | tok/sec: 2466.46\n",
            "step   47 | loss: 4.475117 | lr 5.7600e-04 | norm: 1.9567 | dt: 1654.19ms | tok/sec: 2476.14\n",
            "step   48 | loss: 4.484111 | lr 5.8800e-04 | norm: 1.5218 | dt: 1660.40ms | tok/sec: 2466.88\n",
            "step   49 | loss: 4.654099 | lr 6.0000e-04 | norm: 2.2158 | dt: 1660.93ms | tok/sec: 2466.08\n",
            "step   50 | loss: 4.519504 | lr 6.0000e-04 | norm: 2.4872 | dt: 1670.36ms | tok/sec: 2452.17\n",
            "step   51 | loss: 4.557225 | lr 5.9999e-04 | norm: 1.7073 | dt: 1671.04ms | tok/sec: 2451.16\n",
            "step   52 | loss: 4.297134 | lr 5.9997e-04 | norm: 2.1079 | dt: 1674.36ms | tok/sec: 2446.30\n",
            "step   53 | loss: 4.338378 | lr 5.9994e-04 | norm: 1.6201 | dt: 1687.99ms | tok/sec: 2426.55\n",
            "step   54 | loss: 4.658728 | lr 5.9989e-04 | norm: 2.4125 | dt: 1692.87ms | tok/sec: 2419.56\n",
            "step   55 | loss: 4.193673 | lr 5.9984e-04 | norm: 2.0652 | dt: 1700.17ms | tok/sec: 2409.17\n",
            "step   56 | loss: 4.335505 | lr 5.9976e-04 | norm: 1.8520 | dt: 1702.18ms | tok/sec: 2406.33\n",
            "step   57 | loss: 4.390224 | lr 5.9968e-04 | norm: 1.5454 | dt: 1709.25ms | tok/sec: 2396.37\n",
            "step   58 | loss: 3.809945 | lr 5.9958e-04 | norm: 1.5868 | dt: 1723.06ms | tok/sec: 2377.16\n",
            "step   59 | loss: 4.342382 | lr 5.9947e-04 | norm: 1.7281 | dt: 1722.89ms | tok/sec: 2377.40\n",
            "step   60 | loss: 4.194407 | lr 5.9934e-04 | norm: 1.6476 | dt: 1723.30ms | tok/sec: 2376.83\n",
            "step   61 | loss: 4.134027 | lr 5.9920e-04 | norm: 1.7001 | dt: 1735.08ms | tok/sec: 2360.69\n",
            "step   62 | loss: 4.118028 | lr 5.9905e-04 | norm: 1.5609 | dt: 1731.34ms | tok/sec: 2365.80\n",
            "step   63 | loss: 4.398827 | lr 5.9889e-04 | norm: 1.8788 | dt: 1735.30ms | tok/sec: 2360.40\n",
            "step   64 | loss: 4.264966 | lr 5.9871e-04 | norm: 1.2048 | dt: 1736.14ms | tok/sec: 2359.26\n",
            "step   65 | loss: 4.017577 | lr 5.9852e-04 | norm: 1.2872 | dt: 1734.56ms | tok/sec: 2361.40\n",
            "step   66 | loss: 4.043276 | lr 5.9832e-04 | norm: 1.5590 | dt: 1732.94ms | tok/sec: 2363.61\n",
            "step   67 | loss: 3.999002 | lr 5.9810e-04 | norm: 1.7210 | dt: 1734.24ms | tok/sec: 2361.84\n",
            "step   68 | loss: 4.093080 | lr 5.9787e-04 | norm: 1.2961 | dt: 1719.27ms | tok/sec: 2382.41\n",
            "step   69 | loss: 3.843511 | lr 5.9763e-04 | norm: 1.8313 | dt: 1721.94ms | tok/sec: 2378.71\n",
            "step   70 | loss: 4.030642 | lr 5.9737e-04 | norm: 1.3673 | dt: 1709.58ms | tok/sec: 2395.90\n",
            "step   71 | loss: 4.071179 | lr 5.9710e-04 | norm: 1.3339 | dt: 1705.02ms | tok/sec: 2402.31\n",
            "step   72 | loss: 3.920526 | lr 5.9682e-04 | norm: 1.5691 | dt: 1694.64ms | tok/sec: 2417.03\n",
            "step   73 | loss: 3.832439 | lr 5.9653e-04 | norm: 1.2464 | dt: 1694.85ms | tok/sec: 2416.73\n",
            "step   74 | loss: 3.890844 | lr 5.9622e-04 | norm: 1.2313 | dt: 1701.43ms | tok/sec: 2407.38\n",
            "step   75 | loss: 4.016653 | lr 5.9590e-04 | norm: 1.7918 | dt: 1684.00ms | tok/sec: 2432.31\n",
            "step   76 | loss: 4.008876 | lr 5.9556e-04 | norm: 1.2313 | dt: 1679.80ms | tok/sec: 2438.38\n",
            "step   77 | loss: 4.152074 | lr 5.9522e-04 | norm: 1.7169 | dt: 1680.36ms | tok/sec: 2437.57\n",
            "step   78 | loss: 3.744499 | lr 5.9486e-04 | norm: 1.4385 | dt: 1677.05ms | tok/sec: 2442.38\n",
            "step   79 | loss: 3.958963 | lr 5.9449e-04 | norm: 1.3303 | dt: 1675.31ms | tok/sec: 2444.92\n",
            "step   80 | loss: 4.082405 | lr 5.9410e-04 | norm: 1.0191 | dt: 1672.17ms | tok/sec: 2449.52\n",
            "step   81 | loss: 3.830083 | lr 5.9370e-04 | norm: 1.4926 | dt: 1659.56ms | tok/sec: 2468.13\n",
            "step   82 | loss: 3.870720 | lr 5.9329e-04 | norm: 1.5387 | dt: 1672.63ms | tok/sec: 2448.84\n",
            "step   83 | loss: 4.194841 | lr 5.9287e-04 | norm: 1.3392 | dt: 1657.39ms | tok/sec: 2471.35\n",
            "step   84 | loss: 3.888192 | lr 5.9243e-04 | norm: 1.2689 | dt: 1659.53ms | tok/sec: 2468.17\n",
            "step   85 | loss: 3.867828 | lr 5.9198e-04 | norm: 1.3853 | dt: 1652.03ms | tok/sec: 2479.38\n",
            "step   86 | loss: 3.737628 | lr 5.9152e-04 | norm: 1.4312 | dt: 1655.41ms | tok/sec: 2474.31\n",
            "step   87 | loss: 3.824161 | lr 5.9104e-04 | norm: 1.7912 | dt: 1656.80ms | tok/sec: 2472.24\n",
            "step   88 | loss: 3.496870 | lr 5.9055e-04 | norm: 1.2744 | dt: 1656.74ms | tok/sec: 2472.33\n",
            "step   89 | loss: 3.560991 | lr 5.9005e-04 | norm: 1.3271 | dt: 1655.45ms | tok/sec: 2474.26\n",
            "step   90 | loss: 3.786990 | lr 5.8954e-04 | norm: 1.2950 | dt: 1654.20ms | tok/sec: 2476.13\n",
            "step   91 | loss: 3.742378 | lr 5.8901e-04 | norm: 1.3136 | dt: 1649.96ms | tok/sec: 2482.49\n",
            "step   92 | loss: 3.439579 | lr 5.8848e-04 | norm: 1.4156 | dt: 1655.14ms | tok/sec: 2474.71\n",
            "step   93 | loss: 3.856834 | lr 5.8793e-04 | norm: 1.1812 | dt: 1658.77ms | tok/sec: 2469.29\n",
            "step   94 | loss: 3.908206 | lr 5.8736e-04 | norm: 1.3252 | dt: 1659.45ms | tok/sec: 2468.29\n",
            "step   95 | loss: 3.820768 | lr 5.8679e-04 | norm: 1.1660 | dt: 1656.33ms | tok/sec: 2472.94\n",
            "step   96 | loss: 3.768708 | lr 5.8620e-04 | norm: 1.2628 | dt: 1659.85ms | tok/sec: 2467.70\n",
            "step   97 | loss: 3.586573 | lr 5.8560e-04 | norm: 1.4238 | dt: 1656.68ms | tok/sec: 2472.42\n",
            "step   98 | loss: 3.767621 | lr 5.8498e-04 | norm: 1.2384 | dt: 1661.55ms | tok/sec: 2465.16\n",
            "step   99 | loss: 3.730107 | lr 5.8436e-04 | norm: 1.7957 | dt: 1659.88ms | tok/sec: 2467.65\n",
            "step  100 | loss: 3.588915 | lr 5.8372e-04 | norm: 1.0358 | dt: 1656.34ms | tok/sec: 2472.92\n",
            "step  101 | loss: 3.469483 | lr 5.8307e-04 | norm: 1.1982 | dt: 1658.74ms | tok/sec: 2469.35\n",
            "step  102 | loss: 3.341557 | lr 5.8240e-04 | norm: 1.5749 | dt: 1657.65ms | tok/sec: 2470.98\n",
            "step  103 | loss: 3.391477 | lr 5.8173e-04 | norm: 1.0726 | dt: 1657.77ms | tok/sec: 2470.79\n",
            "step  104 | loss: 3.440986 | lr 5.8104e-04 | norm: 1.4617 | dt: 1670.74ms | tok/sec: 2451.61\n",
            "step  105 | loss: 3.641557 | lr 5.8034e-04 | norm: 1.1261 | dt: 1668.67ms | tok/sec: 2454.64\n",
            "step  106 | loss: 3.311445 | lr 5.7963e-04 | norm: 1.2568 | dt: 1668.29ms | tok/sec: 2455.21\n",
            "step  107 | loss: 3.665374 | lr 5.7890e-04 | norm: 1.1721 | dt: 1674.79ms | tok/sec: 2445.68\n",
            "step  108 | loss: 3.577856 | lr 5.7817e-04 | norm: 1.2258 | dt: 1673.02ms | tok/sec: 2448.27\n",
            "step  109 | loss: 3.468878 | lr 5.7742e-04 | norm: 0.9980 | dt: 1676.15ms | tok/sec: 2443.70\n",
            "step  110 | loss: 3.251907 | lr 5.7666e-04 | norm: 1.5701 | dt: 1681.74ms | tok/sec: 2435.57\n",
            "step  111 | loss: 3.619427 | lr 5.7588e-04 | norm: 1.1341 | dt: 1677.36ms | tok/sec: 2441.93\n",
            "step  112 | loss: 3.476486 | lr 5.7510e-04 | norm: 1.3654 | dt: 1681.16ms | tok/sec: 2436.41\n",
            "step  113 | loss: 3.210561 | lr 5.7430e-04 | norm: 1.0326 | dt: 1688.02ms | tok/sec: 2426.51\n",
            "step  114 | loss: 3.616884 | lr 5.7349e-04 | norm: 1.7689 | dt: 1683.30ms | tok/sec: 2433.31\n",
            "step  115 | loss: 3.216005 | lr 5.7267e-04 | norm: 1.6288 | dt: 1693.80ms | tok/sec: 2418.23\n",
            "step  116 | loss: 3.737504 | lr 5.7184e-04 | norm: 1.3606 | dt: 1690.77ms | tok/sec: 2422.56\n",
            "step  117 | loss: 3.297987 | lr 5.7100e-04 | norm: 1.4978 | dt: 1689.31ms | tok/sec: 2424.66\n",
            "step  118 | loss: 3.512723 | lr 5.7014e-04 | norm: 1.4511 | dt: 1687.81ms | tok/sec: 2426.81\n",
            "step  119 | loss: 3.434142 | lr 5.6927e-04 | norm: 1.6585 | dt: 1687.14ms | tok/sec: 2427.77\n",
            "step  120 | loss: 3.460769 | lr 5.6840e-04 | norm: 1.2861 | dt: 1683.99ms | tok/sec: 2432.31\n",
            "step  121 | loss: 3.483319 | lr 5.6751e-04 | norm: 1.4880 | dt: 1692.83ms | tok/sec: 2419.62\n",
            "step  122 | loss: 3.317451 | lr 5.6660e-04 | norm: 1.1531 | dt: 1686.51ms | tok/sec: 2428.68\n",
            "step  123 | loss: 3.311185 | lr 5.6569e-04 | norm: 1.3287 | dt: 1686.26ms | tok/sec: 2429.05\n",
            "step  124 | loss: 3.211890 | lr 5.6476e-04 | norm: 1.0333 | dt: 1680.75ms | tok/sec: 2437.01\n",
            "step  125 | loss: 3.503842 | lr 5.6383e-04 | norm: 1.3850 | dt: 1681.87ms | tok/sec: 2435.39\n",
            "step  126 | loss: 3.189733 | lr 5.6288e-04 | norm: 1.4353 | dt: 1685.30ms | tok/sec: 2430.43\n",
            "step  127 | loss: 3.626171 | lr 5.6192e-04 | norm: 1.5029 | dt: 1684.80ms | tok/sec: 2431.15\n",
            "step  128 | loss: 3.673798 | lr 5.6095e-04 | norm: 1.6068 | dt: 1676.73ms | tok/sec: 2442.85\n",
            "step  129 | loss: 3.452244 | lr 5.5997e-04 | norm: 1.3994 | dt: 1676.62ms | tok/sec: 2443.01\n",
            "step  130 | loss: 3.441355 | lr 5.5897e-04 | norm: 1.1184 | dt: 1679.35ms | tok/sec: 2439.04\n",
            "step  131 | loss: 3.247053 | lr 5.5797e-04 | norm: 1.4219 | dt: 1674.28ms | tok/sec: 2446.42\n",
            "step  132 | loss: 3.503526 | lr 5.5695e-04 | norm: 1.1133 | dt: 1672.11ms | tok/sec: 2449.59\n",
            "step  133 | loss: 3.338024 | lr 5.5593e-04 | norm: 1.3655 | dt: 1678.85ms | tok/sec: 2439.77\n",
            "step  134 | loss: 3.215827 | lr 5.5489e-04 | norm: 1.5089 | dt: 1676.35ms | tok/sec: 2443.40\n",
            "step  135 | loss: 3.269129 | lr 5.5384e-04 | norm: 1.3566 | dt: 1677.81ms | tok/sec: 2441.27\n",
            "step  136 | loss: 3.042380 | lr 5.5278e-04 | norm: 1.3149 | dt: 1678.16ms | tok/sec: 2440.76\n",
            "step  137 | loss: 3.024482 | lr 5.5171e-04 | norm: 1.0964 | dt: 1676.40ms | tok/sec: 2443.33\n",
            "step  138 | loss: 3.160913 | lr 5.5063e-04 | norm: 1.3484 | dt: 1680.17ms | tok/sec: 2437.85\n",
            "step  139 | loss: 3.077282 | lr 5.4954e-04 | norm: 1.0320 | dt: 1672.97ms | tok/sec: 2448.34\n",
            "step  140 | loss: 3.265605 | lr 5.4843e-04 | norm: 1.2528 | dt: 1677.21ms | tok/sec: 2442.16\n",
            "step  141 | loss: 3.203042 | lr 5.4732e-04 | norm: 1.0351 | dt: 1675.66ms | tok/sec: 2444.41\n",
            "step  142 | loss: 3.085899 | lr 5.4620e-04 | norm: 1.1203 | dt: 1667.26ms | tok/sec: 2456.72\n",
            "step  143 | loss: 3.236900 | lr 5.4506e-04 | norm: 1.4000 | dt: 1668.94ms | tok/sec: 2454.25\n",
            "step  144 | loss: 3.266865 | lr 5.4392e-04 | norm: 1.9251 | dt: 1671.04ms | tok/sec: 2451.17\n",
            "step  145 | loss: 3.090852 | lr 5.4276e-04 | norm: 0.9979 | dt: 1665.62ms | tok/sec: 2459.15\n",
            "step  146 | loss: 3.051875 | lr 5.4160e-04 | norm: 1.2288 | dt: 1666.19ms | tok/sec: 2458.30\n",
            "step  147 | loss: 3.111306 | lr 5.4042e-04 | norm: 1.3660 | dt: 1664.62ms | tok/sec: 2460.62\n",
            "step  148 | loss: 3.092631 | lr 5.3924e-04 | norm: 1.0290 | dt: 1673.28ms | tok/sec: 2447.89\n",
            "step  149 | loss: 3.270474 | lr 5.3804e-04 | norm: 1.2272 | dt: 1678.86ms | tok/sec: 2439.75\n",
            "step  150 | loss: 2.828104 | lr 5.3683e-04 | norm: 1.1442 | dt: 1676.74ms | tok/sec: 2442.84\n",
            "step  151 | loss: 3.239142 | lr 5.3562e-04 | norm: 1.1496 | dt: 1675.33ms | tok/sec: 2444.89\n",
            "step  152 | loss: 2.887869 | lr 5.3439e-04 | norm: 1.6552 | dt: 1673.12ms | tok/sec: 2448.12\n",
            "step  153 | loss: 2.753724 | lr 5.3315e-04 | norm: 1.4493 | dt: 1662.69ms | tok/sec: 2463.48\n",
            "step  154 | loss: 3.019875 | lr 5.3191e-04 | norm: 1.7497 | dt: 1673.24ms | tok/sec: 2447.94\n",
            "step  155 | loss: 3.212679 | lr 5.3065e-04 | norm: 1.1784 | dt: 1672.21ms | tok/sec: 2449.45\n",
            "step  156 | loss: 2.918631 | lr 5.2938e-04 | norm: 1.2137 | dt: 1662.82ms | tok/sec: 2463.28\n",
            "step  157 | loss: 3.078415 | lr 5.2811e-04 | norm: 1.1244 | dt: 1670.67ms | tok/sec: 2451.71\n",
            "step  158 | loss: 3.142013 | lr 5.2682e-04 | norm: 1.2422 | dt: 1679.15ms | tok/sec: 2439.33\n",
            "step  159 | loss: 3.062937 | lr 5.2553e-04 | norm: 1.1693 | dt: 1671.66ms | tok/sec: 2450.26\n",
            "step  160 | loss: 3.145394 | lr 5.2422e-04 | norm: 1.3493 | dt: 1677.79ms | tok/sec: 2441.31\n",
            "step  161 | loss: 3.122278 | lr 5.2291e-04 | norm: 1.2190 | dt: 1667.57ms | tok/sec: 2456.27\n",
            "step  162 | loss: 2.747259 | lr 5.2158e-04 | norm: 1.1126 | dt: 1671.60ms | tok/sec: 2450.35\n",
            "step  163 | loss: 2.984202 | lr 5.2025e-04 | norm: 1.0987 | dt: 1668.15ms | tok/sec: 2455.41\n",
            "step  164 | loss: 2.682400 | lr 5.1891e-04 | norm: 1.3281 | dt: 1662.81ms | tok/sec: 2463.29\n",
            "step  165 | loss: 3.029268 | lr 5.1756e-04 | norm: 1.1422 | dt: 1669.25ms | tok/sec: 2453.79\n",
            "step  166 | loss: 2.872913 | lr 5.1620e-04 | norm: 1.0483 | dt: 1675.91ms | tok/sec: 2444.05\n",
            "step  167 | loss: 2.830352 | lr 5.1483e-04 | norm: 1.1629 | dt: 1672.02ms | tok/sec: 2449.74\n",
            "step  168 | loss: 3.211479 | lr 5.1345e-04 | norm: 1.2159 | dt: 1671.15ms | tok/sec: 2451.01\n",
            "step  169 | loss: 3.113356 | lr 5.1206e-04 | norm: 1.1819 | dt: 1676.16ms | tok/sec: 2443.68\n",
            "step  170 | loss: 2.920963 | lr 5.1067e-04 | norm: 1.2274 | dt: 1669.40ms | tok/sec: 2453.57\n",
            "step  171 | loss: 2.743137 | lr 5.0926e-04 | norm: 1.3250 | dt: 1674.28ms | tok/sec: 2446.43\n",
            "step  172 | loss: 2.763118 | lr 5.0785e-04 | norm: 1.0253 | dt: 1676.96ms | tok/sec: 2442.52\n",
            "step  173 | loss: 3.407274 | lr 5.0642e-04 | norm: 0.9726 | dt: 1673.17ms | tok/sec: 2448.05\n",
            "step  174 | loss: 2.893028 | lr 5.0499e-04 | norm: 1.2397 | dt: 1672.00ms | tok/sec: 2449.76\n",
            "step  175 | loss: 3.126557 | lr 5.0355e-04 | norm: 1.1139 | dt: 1672.92ms | tok/sec: 2448.41\n",
            "step  176 | loss: 2.948172 | lr 5.0210e-04 | norm: 1.0953 | dt: 1674.39ms | tok/sec: 2446.26\n",
            "step  177 | loss: 2.720055 | lr 5.0065e-04 | norm: 1.2729 | dt: 1683.77ms | tok/sec: 2432.64\n",
            "step  178 | loss: 3.030911 | lr 4.9918e-04 | norm: 1.3207 | dt: 1665.63ms | tok/sec: 2459.12\n",
            "step  179 | loss: 2.607360 | lr 4.9771e-04 | norm: 1.5830 | dt: 1676.05ms | tok/sec: 2443.85\n",
            "step  180 | loss: 2.981052 | lr 4.9623e-04 | norm: 1.2558 | dt: 1677.15ms | tok/sec: 2442.23\n",
            "step  181 | loss: 2.934464 | lr 4.9474e-04 | norm: 1.3021 | dt: 1675.26ms | tok/sec: 2444.99\n",
            "step  182 | loss: 2.643539 | lr 4.9324e-04 | norm: 1.1577 | dt: 1676.08ms | tok/sec: 2443.79\n",
            "step  183 | loss: 2.561413 | lr 4.9174e-04 | norm: 1.0700 | dt: 1682.12ms | tok/sec: 2435.03\n",
            "step  184 | loss: 2.854256 | lr 4.9022e-04 | norm: 1.3068 | dt: 1682.60ms | tok/sec: 2434.33\n",
            "step  185 | loss: 2.684810 | lr 4.8870e-04 | norm: 1.3620 | dt: 1671.13ms | tok/sec: 2451.04\n",
            "step  186 | loss: 2.865910 | lr 4.8717e-04 | norm: 1.3625 | dt: 1679.11ms | tok/sec: 2439.39\n",
            "step  187 | loss: 2.797368 | lr 4.8564e-04 | norm: 1.1285 | dt: 1674.16ms | tok/sec: 2446.60\n",
            "step  188 | loss: 2.475828 | lr 4.8409e-04 | norm: 1.1120 | dt: 1674.73ms | tok/sec: 2445.77\n",
            "step  189 | loss: 2.854197 | lr 4.8254e-04 | norm: 1.2455 | dt: 1671.28ms | tok/sec: 2450.82\n",
            "step  190 | loss: 2.717691 | lr 4.8098e-04 | norm: 1.2912 | dt: 1679.92ms | tok/sec: 2438.22\n",
            "step  191 | loss: 3.082420 | lr 4.7942e-04 | norm: 1.7128 | dt: 1676.29ms | tok/sec: 2443.50\n",
            "step  192 | loss: 2.672218 | lr 4.7784e-04 | norm: 1.3091 | dt: 1681.22ms | tok/sec: 2436.33\n",
            "step  193 | loss: 2.692061 | lr 4.7626e-04 | norm: 1.2419 | dt: 1676.03ms | tok/sec: 2443.88\n",
            "step  194 | loss: 2.412967 | lr 4.7467e-04 | norm: 1.6347 | dt: 1671.09ms | tok/sec: 2451.09\n",
            "step  195 | loss: 2.898108 | lr 4.7308e-04 | norm: 2.0365 | dt: 1672.57ms | tok/sec: 2448.92\n",
            "step  196 | loss: 2.539526 | lr 4.7148e-04 | norm: 1.4269 | dt: 1681.27ms | tok/sec: 2436.25\n",
            "step  197 | loss: 2.747336 | lr 4.6987e-04 | norm: 1.2684 | dt: 1677.79ms | tok/sec: 2441.30\n",
            "step  198 | loss: 2.665217 | lr 4.6825e-04 | norm: 1.3612 | dt: 1683.03ms | tok/sec: 2433.70\n",
            "step  199 | loss: 2.784886 | lr 4.6663e-04 | norm: 1.4458 | dt: 1680.32ms | tok/sec: 2437.63\n",
            "step  200 | loss: 2.948347 | lr 4.6500e-04 | norm: 1.1448 | dt: 1669.45ms | tok/sec: 2453.51\n",
            "step  201 | loss: 2.841038 | lr 4.6336e-04 | norm: 1.5936 | dt: 1678.11ms | tok/sec: 2440.84\n",
            "step  202 | loss: 2.809943 | lr 4.6172e-04 | norm: 1.1205 | dt: 1666.90ms | tok/sec: 2457.25\n",
            "step  203 | loss: 2.798070 | lr 4.6007e-04 | norm: 1.2244 | dt: 1677.33ms | tok/sec: 2441.98\n",
            "step  204 | loss: 2.883968 | lr 4.5842e-04 | norm: 1.2364 | dt: 1675.51ms | tok/sec: 2444.63\n",
            "step  205 | loss: 2.758349 | lr 4.5676e-04 | norm: 1.1889 | dt: 1682.74ms | tok/sec: 2434.13\n",
            "step  206 | loss: 2.656917 | lr 4.5509e-04 | norm: 1.6029 | dt: 1673.05ms | tok/sec: 2448.22\n",
            "step  207 | loss: 2.621903 | lr 4.5342e-04 | norm: 1.4563 | dt: 1676.69ms | tok/sec: 2442.91\n",
            "step  208 | loss: 2.420525 | lr 4.5174e-04 | norm: 1.2099 | dt: 1681.01ms | tok/sec: 2436.63\n",
            "step  209 | loss: 2.494337 | lr 4.5005e-04 | norm: 1.5438 | dt: 1684.08ms | tok/sec: 2432.19\n",
            "step  210 | loss: 2.568618 | lr 4.4836e-04 | norm: 1.3809 | dt: 1677.63ms | tok/sec: 2441.53\n",
            "step  211 | loss: 2.687308 | lr 4.4666e-04 | norm: 1.5308 | dt: 1674.13ms | tok/sec: 2446.64\n",
            "step  212 | loss: 2.664806 | lr 4.4496e-04 | norm: 1.3376 | dt: 1677.22ms | tok/sec: 2442.13\n",
            "step  213 | loss: 2.144941 | lr 4.4325e-04 | norm: 1.1342 | dt: 1675.88ms | tok/sec: 2444.10\n",
            "step  214 | loss: 2.328884 | lr 4.4154e-04 | norm: 1.3068 | dt: 1680.24ms | tok/sec: 2437.75\n",
            "step  215 | loss: 2.644977 | lr 4.3982e-04 | norm: 1.4072 | dt: 1677.24ms | tok/sec: 2442.10\n",
            "step  216 | loss: 2.515424 | lr 4.3809e-04 | norm: 1.0277 | dt: 1678.70ms | tok/sec: 2439.98\n",
            "step  217 | loss: 2.487322 | lr 4.3636e-04 | norm: 1.5352 | dt: 1677.80ms | tok/sec: 2441.29\n",
            "step  218 | loss: 2.641198 | lr 4.3463e-04 | norm: 1.3748 | dt: 1679.80ms | tok/sec: 2438.38\n",
            "step  219 | loss: 2.819960 | lr 4.3289e-04 | norm: 1.3895 | dt: 1675.96ms | tok/sec: 2443.97\n",
            "step  220 | loss: 2.336278 | lr 4.3114e-04 | norm: 1.2446 | dt: 1674.09ms | tok/sec: 2446.70\n",
            "step  221 | loss: 2.617258 | lr 4.2939e-04 | norm: 1.3642 | dt: 1673.96ms | tok/sec: 2446.89\n",
            "step  222 | loss: 2.326418 | lr 4.2764e-04 | norm: 1.1295 | dt: 1671.49ms | tok/sec: 2450.50\n",
            "step  223 | loss: 2.291808 | lr 4.2588e-04 | norm: 1.0913 | dt: 1680.22ms | tok/sec: 2437.78\n",
            "step  224 | loss: 2.150715 | lr 4.2411e-04 | norm: 1.0669 | dt: 1680.41ms | tok/sec: 2437.50\n",
            "step  225 | loss: 2.560931 | lr 4.2235e-04 | norm: 1.2522 | dt: 1683.79ms | tok/sec: 2432.61\n",
            "step  226 | loss: 2.272482 | lr 4.2057e-04 | norm: 1.0916 | dt: 1678.06ms | tok/sec: 2440.91\n",
            "step  227 | loss: 2.432564 | lr 4.1879e-04 | norm: 1.1423 | dt: 1681.11ms | tok/sec: 2436.49\n",
            "step  228 | loss: 2.525895 | lr 4.1701e-04 | norm: 1.2198 | dt: 1670.71ms | tok/sec: 2451.65\n",
            "step  229 | loss: 2.402712 | lr 4.1523e-04 | norm: 1.1453 | dt: 1680.94ms | tok/sec: 2436.73\n",
            "step  230 | loss: 2.705953 | lr 4.1343e-04 | norm: 1.3089 | dt: 1682.34ms | tok/sec: 2434.71\n",
            "step  231 | loss: 2.356493 | lr 4.1164e-04 | norm: 1.4291 | dt: 1678.08ms | tok/sec: 2440.88\n",
            "step  232 | loss: 2.435394 | lr 4.0984e-04 | norm: 1.4334 | dt: 1679.56ms | tok/sec: 2438.73\n",
            "step  233 | loss: 2.187825 | lr 4.0804e-04 | norm: 1.3080 | dt: 1675.90ms | tok/sec: 2444.06\n",
            "step  234 | loss: 2.261450 | lr 4.0623e-04 | norm: 1.2307 | dt: 1680.91ms | tok/sec: 2436.78\n",
            "step  235 | loss: 2.602840 | lr 4.0442e-04 | norm: 1.6774 | dt: 1674.47ms | tok/sec: 2446.15\n",
            "step  236 | loss: 2.398436 | lr 4.0261e-04 | norm: 1.4657 | dt: 1672.60ms | tok/sec: 2448.88\n",
            "step  237 | loss: 2.422107 | lr 4.0079e-04 | norm: 1.3259 | dt: 1683.73ms | tok/sec: 2432.70\n",
            "step  238 | loss: 2.353002 | lr 3.9897e-04 | norm: 1.6043 | dt: 1670.71ms | tok/sec: 2451.65\n",
            "step  239 | loss: 2.316969 | lr 3.9715e-04 | norm: 1.5736 | dt: 1677.67ms | tok/sec: 2441.49\n",
            "step  240 | loss: 2.345347 | lr 3.9532e-04 | norm: 1.8660 | dt: 1682.86ms | tok/sec: 2433.95\n",
            "step  241 | loss: 2.508341 | lr 3.9349e-04 | norm: 1.3072 | dt: 1676.33ms | tok/sec: 2443.43\n",
            "step  242 | loss: 2.473391 | lr 3.9165e-04 | norm: 1.2227 | dt: 1678.97ms | tok/sec: 2439.59\n",
            "step  243 | loss: 2.150867 | lr 3.8982e-04 | norm: 1.2592 | dt: 1684.69ms | tok/sec: 2431.30\n",
            "step  244 | loss: 2.349644 | lr 3.8798e-04 | norm: 1.1740 | dt: 1676.99ms | tok/sec: 2442.47\n",
            "step  245 | loss: 2.384147 | lr 3.8614e-04 | norm: 1.2403 | dt: 1678.92ms | tok/sec: 2439.66\n",
            "step  246 | loss: 2.179541 | lr 3.8429e-04 | norm: 1.3751 | dt: 1675.91ms | tok/sec: 2444.04\n",
            "step  247 | loss: 2.308445 | lr 3.8244e-04 | norm: 1.5709 | dt: 1690.83ms | tok/sec: 2422.48\n",
            "step  248 | loss: 2.199338 | lr 3.8059e-04 | norm: 1.6314 | dt: 1675.12ms | tok/sec: 2445.20\n",
            "step  249 | loss: 2.142970 | lr 3.7874e-04 | norm: 1.5017 | dt: 1679.54ms | tok/sec: 2438.76\n",
            "step  250 | loss: 2.076366 | lr 3.7689e-04 | norm: 1.5618 | dt: 1684.86ms | tok/sec: 2431.06\n",
            "step  251 | loss: 2.221214 | lr 3.7503e-04 | norm: 1.2031 | dt: 1674.16ms | tok/sec: 2446.61\n",
            "step  252 | loss: 2.369893 | lr 3.7317e-04 | norm: 1.3983 | dt: 1686.63ms | tok/sec: 2428.52\n",
            "step  253 | loss: 2.290576 | lr 3.7131e-04 | norm: 1.4814 | dt: 1676.20ms | tok/sec: 2443.63\n",
            "step  254 | loss: 2.281464 | lr 3.6944e-04 | norm: 1.4231 | dt: 1675.84ms | tok/sec: 2444.14\n",
            "step  255 | loss: 2.417538 | lr 3.6758e-04 | norm: 1.2810 | dt: 1678.58ms | tok/sec: 2440.15\n",
            "step  256 | loss: 2.121938 | lr 3.6571e-04 | norm: 1.3605 | dt: 1677.79ms | tok/sec: 2441.30\n",
            "step  257 | loss: 2.291906 | lr 3.6384e-04 | norm: 1.2872 | dt: 1679.81ms | tok/sec: 2438.37\n",
            "step  258 | loss: 2.206798 | lr 3.6197e-04 | norm: 1.3451 | dt: 1676.86ms | tok/sec: 2442.66\n",
            "step  259 | loss: 2.201957 | lr 3.6010e-04 | norm: 1.3055 | dt: 1679.39ms | tok/sec: 2438.98\n",
            "step  260 | loss: 2.081944 | lr 3.5822e-04 | norm: 1.1563 | dt: 1678.60ms | tok/sec: 2440.13\n",
            "step  261 | loss: 1.983642 | lr 3.5635e-04 | norm: 1.4717 | dt: 1673.32ms | tok/sec: 2447.82\n",
            "step  262 | loss: 2.109421 | lr 3.5447e-04 | norm: 1.3100 | dt: 1673.65ms | tok/sec: 2447.34\n",
            "step  263 | loss: 2.437372 | lr 3.5259e-04 | norm: 1.4642 | dt: 1679.20ms | tok/sec: 2439.26\n",
            "step  264 | loss: 2.220490 | lr 3.5071e-04 | norm: 1.2901 | dt: 1678.89ms | tok/sec: 2439.71\n",
            "step  265 | loss: 2.186837 | lr 3.4883e-04 | norm: 1.4688 | dt: 1685.68ms | tok/sec: 2429.88\n",
            "step  266 | loss: 2.347154 | lr 3.4695e-04 | norm: 1.3968 | dt: 1679.86ms | tok/sec: 2438.30\n",
            "step  267 | loss: 2.326182 | lr 3.4507e-04 | norm: 1.2928 | dt: 1681.84ms | tok/sec: 2435.43\n",
            "step  268 | loss: 2.153880 | lr 3.4319e-04 | norm: 1.1621 | dt: 1673.20ms | tok/sec: 2448.01\n",
            "step  269 | loss: 1.839926 | lr 3.4131e-04 | norm: 1.4813 | dt: 1673.77ms | tok/sec: 2447.16\n",
            "step  270 | loss: 2.081065 | lr 3.3942e-04 | norm: 1.3919 | dt: 1677.71ms | tok/sec: 2441.42\n",
            "step  271 | loss: 1.976332 | lr 3.3754e-04 | norm: 1.2014 | dt: 1672.16ms | tok/sec: 2449.53\n",
            "step  272 | loss: 1.904680 | lr 3.3565e-04 | norm: 1.5059 | dt: 1676.10ms | tok/sec: 2443.76\n",
            "step  273 | loss: 2.025209 | lr 3.3377e-04 | norm: 1.2786 | dt: 1676.17ms | tok/sec: 2443.67\n",
            "step  274 | loss: 2.168061 | lr 3.3188e-04 | norm: 1.3908 | dt: 1677.40ms | tok/sec: 2441.87\n",
            "step  275 | loss: 2.183666 | lr 3.3000e-04 | norm: 1.3293 | dt: 1675.91ms | tok/sec: 2444.04\n",
            "step  276 | loss: 2.154497 | lr 3.2812e-04 | norm: 1.3954 | dt: 1680.68ms | tok/sec: 2437.10\n",
            "step  277 | loss: 1.896069 | lr 3.2623e-04 | norm: 1.2482 | dt: 1672.95ms | tok/sec: 2448.37\n",
            "step  278 | loss: 1.904283 | lr 3.2435e-04 | norm: 1.5556 | dt: 1678.54ms | tok/sec: 2440.21\n",
            "step  279 | loss: 2.018294 | lr 3.2246e-04 | norm: 1.6078 | dt: 1674.61ms | tok/sec: 2445.94\n",
            "step  280 | loss: 1.959110 | lr 3.2058e-04 | norm: 1.3320 | dt: 1685.25ms | tok/sec: 2430.51\n",
            "step  281 | loss: 2.047387 | lr 3.1869e-04 | norm: 1.3255 | dt: 1681.96ms | tok/sec: 2435.26\n",
            "step  282 | loss: 2.274405 | lr 3.1681e-04 | norm: 1.7913 | dt: 1671.92ms | tok/sec: 2449.87\n",
            "step  283 | loss: 1.740581 | lr 3.1493e-04 | norm: 1.2254 | dt: 1670.69ms | tok/sec: 2451.68\n",
            "step  284 | loss: 1.921225 | lr 3.1305e-04 | norm: 1.4227 | dt: 1676.27ms | tok/sec: 2443.52\n",
            "step  285 | loss: 1.897992 | lr 3.1117e-04 | norm: 1.5659 | dt: 1677.63ms | tok/sec: 2441.54\n",
            "step  286 | loss: 1.982591 | lr 3.0929e-04 | norm: 1.4348 | dt: 1682.41ms | tok/sec: 2434.60\n",
            "step  287 | loss: 2.016887 | lr 3.0741e-04 | norm: 1.6248 | dt: 1676.46ms | tok/sec: 2443.25\n",
            "step  288 | loss: 1.861006 | lr 3.0553e-04 | norm: 1.4599 | dt: 1676.80ms | tok/sec: 2442.75\n",
            "step  289 | loss: 2.073422 | lr 3.0365e-04 | norm: 1.3887 | dt: 1678.05ms | tok/sec: 2440.93\n",
            "step  290 | loss: 2.047395 | lr 3.0178e-04 | norm: 1.3997 | dt: 1679.02ms | tok/sec: 2439.52\n",
            "step  291 | loss: 1.987693 | lr 2.9990e-04 | norm: 1.5109 | dt: 1675.81ms | tok/sec: 2444.19\n",
            "step  292 | loss: 1.880695 | lr 2.9803e-04 | norm: 1.2826 | dt: 1681.35ms | tok/sec: 2436.14\n",
            "step  293 | loss: 1.822655 | lr 2.9616e-04 | norm: 1.5410 | dt: 1672.83ms | tok/sec: 2448.55\n",
            "step  294 | loss: 1.969312 | lr 2.9429e-04 | norm: 1.6792 | dt: 1679.54ms | tok/sec: 2438.76\n",
            "step  295 | loss: 1.890389 | lr 2.9242e-04 | norm: 1.4681 | dt: 1677.69ms | tok/sec: 2441.45\n",
            "step  296 | loss: 1.957282 | lr 2.9056e-04 | norm: 1.4039 | dt: 1674.14ms | tok/sec: 2446.64\n",
            "step  297 | loss: 1.810302 | lr 2.8869e-04 | norm: 1.4002 | dt: 1679.11ms | tok/sec: 2439.38\n",
            "step  298 | loss: 1.901231 | lr 2.8683e-04 | norm: 1.4140 | dt: 1674.95ms | tok/sec: 2445.44\n",
            "step  299 | loss: 1.896393 | lr 2.8497e-04 | norm: 1.3517 | dt: 1679.41ms | tok/sec: 2438.94\n",
            "step  300 | loss: 1.805864 | lr 2.8311e-04 | norm: 1.1673 | dt: 1683.08ms | tok/sec: 2433.64\n",
            "step  301 | loss: 2.082191 | lr 2.8126e-04 | norm: 1.5690 | dt: 1675.88ms | tok/sec: 2444.08\n",
            "step  302 | loss: 1.891922 | lr 2.7941e-04 | norm: 1.5978 | dt: 1677.43ms | tok/sec: 2441.83\n",
            "step  303 | loss: 1.838027 | lr 2.7756e-04 | norm: 1.2358 | dt: 1679.26ms | tok/sec: 2439.17\n",
            "step  304 | loss: 1.690303 | lr 2.7571e-04 | norm: 1.5994 | dt: 1676.26ms | tok/sec: 2443.53\n",
            "step  305 | loss: 1.817018 | lr 2.7386e-04 | norm: 1.4653 | dt: 1674.24ms | tok/sec: 2446.48\n",
            "step  306 | loss: 1.975279 | lr 2.7202e-04 | norm: 1.4354 | dt: 1678.07ms | tok/sec: 2440.90\n",
            "step  307 | loss: 1.844744 | lr 2.7018e-04 | norm: 1.8919 | dt: 1680.39ms | tok/sec: 2437.53\n",
            "step  308 | loss: 1.802983 | lr 2.6835e-04 | norm: 1.2553 | dt: 1679.99ms | tok/sec: 2438.10\n",
            "step  309 | loss: 1.688968 | lr 2.6651e-04 | norm: 1.4768 | dt: 1687.65ms | tok/sec: 2427.04\n",
            "step  310 | loss: 1.722235 | lr 2.6468e-04 | norm: 1.5021 | dt: 1677.80ms | tok/sec: 2441.29\n",
            "step  311 | loss: 1.750841 | lr 2.6285e-04 | norm: 1.3982 | dt: 1688.52ms | tok/sec: 2425.79\n",
            "step  312 | loss: 1.812785 | lr 2.6103e-04 | norm: 1.3501 | dt: 1672.57ms | tok/sec: 2448.92\n",
            "step  313 | loss: 1.643128 | lr 2.5921e-04 | norm: 1.4639 | dt: 1684.20ms | tok/sec: 2432.02\n",
            "step  314 | loss: 1.683409 | lr 2.5739e-04 | norm: 1.3409 | dt: 1684.43ms | tok/sec: 2431.68\n",
            "step  315 | loss: 2.038145 | lr 2.5558e-04 | norm: 1.5682 | dt: 1678.09ms | tok/sec: 2440.86\n",
            "step  316 | loss: 1.674979 | lr 2.5377e-04 | norm: 1.2552 | dt: 1682.24ms | tok/sec: 2434.86\n",
            "step  317 | loss: 1.663463 | lr 2.5196e-04 | norm: 1.3042 | dt: 1679.11ms | tok/sec: 2439.39\n",
            "step  318 | loss: 1.614685 | lr 2.5016e-04 | norm: 1.4484 | dt: 1678.78ms | tok/sec: 2439.87\n",
            "step  319 | loss: 1.944615 | lr 2.4836e-04 | norm: 1.5183 | dt: 1678.57ms | tok/sec: 2440.17\n",
            "step  320 | loss: 1.639583 | lr 2.4657e-04 | norm: 1.4129 | dt: 1679.07ms | tok/sec: 2439.44\n",
            "step  321 | loss: 1.600632 | lr 2.4477e-04 | norm: 1.3951 | dt: 1675.80ms | tok/sec: 2444.20\n",
            "step  322 | loss: 1.774770 | lr 2.4299e-04 | norm: 1.1863 | dt: 1681.19ms | tok/sec: 2436.37\n",
            "step  323 | loss: 1.562040 | lr 2.4121e-04 | norm: 1.3845 | dt: 1685.39ms | tok/sec: 2430.29\n",
            "step  324 | loss: 1.625272 | lr 2.3943e-04 | norm: 1.4019 | dt: 1678.12ms | tok/sec: 2440.82\n",
            "step  325 | loss: 1.655259 | lr 2.3765e-04 | norm: 1.4186 | dt: 1678.33ms | tok/sec: 2440.52\n",
            "step  326 | loss: 1.602798 | lr 2.3589e-04 | norm: 1.4130 | dt: 1687.64ms | tok/sec: 2427.06\n",
            "step  327 | loss: 1.685892 | lr 2.3412e-04 | norm: 1.5477 | dt: 1679.93ms | tok/sec: 2438.19\n",
            "step  328 | loss: 1.432753 | lr 2.3236e-04 | norm: 1.1336 | dt: 1677.87ms | tok/sec: 2441.20\n",
            "step  329 | loss: 1.536611 | lr 2.3061e-04 | norm: 1.1255 | dt: 1677.17ms | tok/sec: 2442.20\n",
            "step  330 | loss: 1.488342 | lr 2.2886e-04 | norm: 1.2901 | dt: 1681.65ms | tok/sec: 2435.71\n",
            "step  331 | loss: 1.440733 | lr 2.2711e-04 | norm: 1.2536 | dt: 1674.31ms | tok/sec: 2446.38\n",
            "step  332 | loss: 1.547477 | lr 2.2537e-04 | norm: 1.3940 | dt: 1684.10ms | tok/sec: 2432.16\n",
            "step  333 | loss: 1.465942 | lr 2.2364e-04 | norm: 1.3045 | dt: 1676.40ms | tok/sec: 2443.33\n",
            "step  334 | loss: 1.670959 | lr 2.2191e-04 | norm: 1.3935 | dt: 1682.17ms | tok/sec: 2434.94\n",
            "step  335 | loss: 1.523907 | lr 2.2018e-04 | norm: 1.3713 | dt: 1677.79ms | tok/sec: 2441.31\n",
            "step  336 | loss: 1.528016 | lr 2.1846e-04 | norm: 1.2585 | dt: 1676.12ms | tok/sec: 2443.74\n",
            "step  337 | loss: 1.533156 | lr 2.1675e-04 | norm: 1.3247 | dt: 1677.62ms | tok/sec: 2441.55\n",
            "step  338 | loss: 1.275788 | lr 2.1504e-04 | norm: 1.1192 | dt: 1682.55ms | tok/sec: 2434.40\n",
            "step  339 | loss: 1.539465 | lr 2.1334e-04 | norm: 1.5590 | dt: 1684.28ms | tok/sec: 2431.89\n",
            "step  340 | loss: 1.397255 | lr 2.1164e-04 | norm: 1.2362 | dt: 1681.27ms | tok/sec: 2436.25\n",
            "step  341 | loss: 1.397246 | lr 2.0995e-04 | norm: 1.0909 | dt: 1673.98ms | tok/sec: 2446.87\n",
            "step  342 | loss: 1.499385 | lr 2.0826e-04 | norm: 1.3695 | dt: 1674.08ms | tok/sec: 2446.72\n",
            "step  343 | loss: 1.492062 | lr 2.0658e-04 | norm: 1.4743 | dt: 1675.75ms | tok/sec: 2444.28\n",
            "step  344 | loss: 1.356487 | lr 2.0491e-04 | norm: 1.2150 | dt: 1683.47ms | tok/sec: 2433.07\n",
            "step  345 | loss: 1.514688 | lr 2.0324e-04 | norm: 1.5113 | dt: 1675.21ms | tok/sec: 2445.06\n",
            "step  346 | loss: 1.378102 | lr 2.0158e-04 | norm: 1.2934 | dt: 1675.01ms | tok/sec: 2445.36\n",
            "step  347 | loss: 1.310270 | lr 1.9993e-04 | norm: 1.1772 | dt: 1674.97ms | tok/sec: 2445.41\n",
            "step  348 | loss: 1.478393 | lr 1.9828e-04 | norm: 1.2049 | dt: 1679.54ms | tok/sec: 2438.76\n",
            "step  349 | loss: 1.639695 | lr 1.9664e-04 | norm: 1.4590 | dt: 1677.85ms | tok/sec: 2441.22\n",
            "step  350 | loss: 1.452435 | lr 1.9500e-04 | norm: 1.4663 | dt: 1678.04ms | tok/sec: 2440.94\n",
            "step  351 | loss: 1.426263 | lr 1.9337e-04 | norm: 1.3043 | dt: 1686.00ms | tok/sec: 2429.43\n",
            "step  352 | loss: 1.394238 | lr 1.9175e-04 | norm: 1.3383 | dt: 1677.80ms | tok/sec: 2441.29\n",
            "step  353 | loss: 1.400432 | lr 1.9013e-04 | norm: 1.2764 | dt: 1683.40ms | tok/sec: 2433.17\n",
            "step  354 | loss: 1.414370 | lr 1.8852e-04 | norm: 1.3871 | dt: 1680.71ms | tok/sec: 2437.07\n",
            "step  355 | loss: 1.572810 | lr 1.8692e-04 | norm: 1.2871 | dt: 1676.98ms | tok/sec: 2442.48\n",
            "step  356 | loss: 1.540467 | lr 1.8533e-04 | norm: 1.4565 | dt: 1675.68ms | tok/sec: 2444.38\n",
            "step  357 | loss: 1.313135 | lr 1.8374e-04 | norm: 1.4381 | dt: 1680.47ms | tok/sec: 2437.42\n",
            "step  358 | loss: 1.251015 | lr 1.8216e-04 | norm: 1.2581 | dt: 1677.73ms | tok/sec: 2441.39\n",
            "step  359 | loss: 1.491014 | lr 1.8058e-04 | norm: 1.3916 | dt: 1680.37ms | tok/sec: 2437.56\n",
            "step  360 | loss: 1.449213 | lr 1.7902e-04 | norm: 1.4370 | dt: 1678.85ms | tok/sec: 2439.76\n",
            "step  361 | loss: 1.335230 | lr 1.7746e-04 | norm: 1.3556 | dt: 1676.94ms | tok/sec: 2442.55\n",
            "step  362 | loss: 1.367361 | lr 1.7591e-04 | norm: 1.3051 | dt: 1678.51ms | tok/sec: 2440.25\n",
            "step  363 | loss: 1.271030 | lr 1.7436e-04 | norm: 1.2456 | dt: 1675.12ms | tok/sec: 2445.20\n",
            "step  364 | loss: 1.299405 | lr 1.7283e-04 | norm: 1.2259 | dt: 1675.06ms | tok/sec: 2445.28\n",
            "step  365 | loss: 1.425937 | lr 1.7130e-04 | norm: 1.3647 | dt: 1685.91ms | tok/sec: 2429.54\n",
            "step  366 | loss: 1.261692 | lr 1.6978e-04 | norm: 1.5573 | dt: 1679.05ms | tok/sec: 2439.48\n",
            "step  367 | loss: 1.249502 | lr 1.6826e-04 | norm: 1.2269 | dt: 1680.10ms | tok/sec: 2437.95\n",
            "step  368 | loss: 1.321369 | lr 1.6676e-04 | norm: 1.4665 | dt: 1675.73ms | tok/sec: 2444.31\n",
            "step  369 | loss: 1.240192 | lr 1.6526e-04 | norm: 1.2813 | dt: 1679.14ms | tok/sec: 2439.34\n",
            "step  370 | loss: 1.197865 | lr 1.6377e-04 | norm: 1.1857 | dt: 1681.43ms | tok/sec: 2436.02\n",
            "step  371 | loss: 1.419721 | lr 1.6229e-04 | norm: 1.3680 | dt: 1683.01ms | tok/sec: 2433.74\n",
            "step  372 | loss: 1.330458 | lr 1.6082e-04 | norm: 1.4032 | dt: 1677.91ms | tok/sec: 2441.14\n",
            "step  373 | loss: 1.243103 | lr 1.5935e-04 | norm: 1.3199 | dt: 1680.79ms | tok/sec: 2436.95\n",
            "step  374 | loss: 1.262967 | lr 1.5790e-04 | norm: 1.3220 | dt: 1672.28ms | tok/sec: 2449.35\n",
            "step  375 | loss: 1.199921 | lr 1.5645e-04 | norm: 1.1778 | dt: 1679.05ms | tok/sec: 2439.47\n",
            "step  376 | loss: 1.151846 | lr 1.5501e-04 | norm: 1.0756 | dt: 1680.76ms | tok/sec: 2436.99\n",
            "step  377 | loss: 1.270716 | lr 1.5358e-04 | norm: 1.6718 | dt: 1685.68ms | tok/sec: 2429.88\n",
            "step  378 | loss: 1.200432 | lr 1.5215e-04 | norm: 1.2165 | dt: 1690.14ms | tok/sec: 2423.47\n",
            "step  379 | loss: 1.194681 | lr 1.5074e-04 | norm: 1.3004 | dt: 1677.51ms | tok/sec: 2441.71\n",
            "step  380 | loss: 1.154131 | lr 1.4933e-04 | norm: 1.2053 | dt: 1679.07ms | tok/sec: 2439.45\n",
            "step  381 | loss: 1.411376 | lr 1.4794e-04 | norm: 1.3964 | dt: 1680.16ms | tok/sec: 2437.86\n",
            "step  382 | loss: 1.202530 | lr 1.4655e-04 | norm: 1.2197 | dt: 1683.98ms | tok/sec: 2432.33\n",
            "step  383 | loss: 1.131912 | lr 1.4517e-04 | norm: 1.2579 | dt: 1678.46ms | tok/sec: 2440.34\n",
            "step  384 | loss: 1.293095 | lr 1.4380e-04 | norm: 1.4945 | dt: 1684.48ms | tok/sec: 2431.61\n",
            "step  385 | loss: 1.280545 | lr 1.4244e-04 | norm: 1.3681 | dt: 1683.82ms | tok/sec: 2432.57\n",
            "step  386 | loss: 1.275773 | lr 1.4109e-04 | norm: 1.2638 | dt: 1686.29ms | tok/sec: 2429.00\n",
            "step  387 | loss: 1.095554 | lr 1.3975e-04 | norm: 1.2280 | dt: 1679.33ms | tok/sec: 2439.06\n",
            "step  388 | loss: 1.203928 | lr 1.3842e-04 | norm: 1.3334 | dt: 1679.92ms | tok/sec: 2438.22\n",
            "step  389 | loss: 1.138267 | lr 1.3709e-04 | norm: 1.2693 | dt: 1676.04ms | tok/sec: 2443.86\n",
            "step  390 | loss: 1.058771 | lr 1.3578e-04 | norm: 1.1533 | dt: 1684.88ms | tok/sec: 2431.03\n",
            "step  391 | loss: 1.180942 | lr 1.3447e-04 | norm: 1.4425 | dt: 1680.31ms | tok/sec: 2437.65\n",
            "step  392 | loss: 1.140453 | lr 1.3318e-04 | norm: 1.1869 | dt: 1693.12ms | tok/sec: 2419.21\n",
            "step  393 | loss: 1.088004 | lr 1.3189e-04 | norm: 1.3633 | dt: 1683.69ms | tok/sec: 2432.75\n",
            "step  394 | loss: 1.280061 | lr 1.3062e-04 | norm: 1.2846 | dt: 1682.10ms | tok/sec: 2435.05\n",
            "step  395 | loss: 1.096339 | lr 1.2935e-04 | norm: 1.3794 | dt: 1688.81ms | tok/sec: 2425.38\n",
            "step  396 | loss: 1.039813 | lr 1.2809e-04 | norm: 1.2291 | dt: 1689.33ms | tok/sec: 2424.63\n",
            "step  397 | loss: 1.142272 | lr 1.2685e-04 | norm: 1.0875 | dt: 1671.70ms | tok/sec: 2450.20\n",
            "step  398 | loss: 1.137717 | lr 1.2561e-04 | norm: 1.1866 | dt: 1670.30ms | tok/sec: 2452.26\n",
            "step  399 | loss: 1.036315 | lr 1.2438e-04 | norm: 1.1830 | dt: 1680.60ms | tok/sec: 2437.23\n",
            "step  400 | loss: 1.120140 | lr 1.2317e-04 | norm: 1.1871 | dt: 1684.88ms | tok/sec: 2431.03\n",
            "step  401 | loss: 1.084044 | lr 1.2196e-04 | norm: 1.0603 | dt: 1677.69ms | tok/sec: 2441.46\n",
            "step  402 | loss: 1.155122 | lr 1.2076e-04 | norm: 1.3345 | dt: 1678.65ms | tok/sec: 2440.05\n",
            "step  403 | loss: 1.216442 | lr 1.1958e-04 | norm: 1.2720 | dt: 1678.76ms | tok/sec: 2439.89\n",
            "step  404 | loss: 1.182326 | lr 1.1840e-04 | norm: 1.3915 | dt: 1681.01ms | tok/sec: 2436.63\n",
            "step  405 | loss: 1.144604 | lr 1.1724e-04 | norm: 1.2182 | dt: 1680.37ms | tok/sec: 2437.56\n",
            "step  406 | loss: 1.183639 | lr 1.1608e-04 | norm: 1.3298 | dt: 1676.24ms | tok/sec: 2443.56\n",
            "step  407 | loss: 1.171586 | lr 1.1494e-04 | norm: 1.4202 | dt: 1687.10ms | tok/sec: 2427.83\n",
            "step  408 | loss: 1.128029 | lr 1.1380e-04 | norm: 1.2903 | dt: 1685.80ms | tok/sec: 2429.71\n",
            "step  409 | loss: 1.128814 | lr 1.1268e-04 | norm: 1.2045 | dt: 1678.36ms | tok/sec: 2440.48\n",
            "step  410 | loss: 1.036875 | lr 1.1157e-04 | norm: 1.1379 | dt: 1682.85ms | tok/sec: 2433.97\n",
            "step  411 | loss: 1.024408 | lr 1.1046e-04 | norm: 1.1592 | dt: 1682.90ms | tok/sec: 2433.89\n",
            "step  412 | loss: 0.999129 | lr 1.0937e-04 | norm: 1.1728 | dt: 1682.68ms | tok/sec: 2434.22\n",
            "step  413 | loss: 0.989848 | lr 1.0829e-04 | norm: 1.1368 | dt: 1687.83ms | tok/sec: 2426.79\n",
            "step  414 | loss: 0.897179 | lr 1.0722e-04 | norm: 1.0591 | dt: 1681.78ms | tok/sec: 2435.52\n",
            "step  415 | loss: 1.139015 | lr 1.0616e-04 | norm: 1.2109 | dt: 1678.34ms | tok/sec: 2440.50\n",
            "step  416 | loss: 1.019860 | lr 1.0511e-04 | norm: 1.0339 | dt: 1678.11ms | tok/sec: 2440.84\n",
            "step  417 | loss: 0.970832 | lr 1.0407e-04 | norm: 1.2596 | dt: 1674.84ms | tok/sec: 2445.61\n",
            "step  418 | loss: 1.148223 | lr 1.0305e-04 | norm: 1.2573 | dt: 1678.40ms | tok/sec: 2440.42\n",
            "step  419 | loss: 1.071760 | lr 1.0203e-04 | norm: 1.3314 | dt: 1692.69ms | tok/sec: 2419.82\n",
            "step  420 | loss: 0.998918 | lr 1.0103e-04 | norm: 1.1388 | dt: 1679.51ms | tok/sec: 2438.81\n",
            "step  421 | loss: 1.082000 | lr 1.0003e-04 | norm: 1.2228 | dt: 1679.74ms | tok/sec: 2438.47\n",
            "step  422 | loss: 0.971614 | lr 9.9052e-05 | norm: 1.1292 | dt: 1676.93ms | tok/sec: 2442.56\n",
            "step  423 | loss: 1.072186 | lr 9.8081e-05 | norm: 1.2905 | dt: 1689.36ms | tok/sec: 2424.59\n",
            "step  424 | loss: 1.046623 | lr 9.7121e-05 | norm: 1.2188 | dt: 1682.30ms | tok/sec: 2434.76\n",
            "step  425 | loss: 1.111461 | lr 9.6173e-05 | norm: 1.2727 | dt: 1688.58ms | tok/sec: 2425.71\n",
            "step  426 | loss: 0.967529 | lr 9.5236e-05 | norm: 1.1359 | dt: 1686.53ms | tok/sec: 2428.66\n",
            "step  427 | loss: 0.900600 | lr 9.4311e-05 | norm: 1.1313 | dt: 1681.37ms | tok/sec: 2436.11\n",
            "step  428 | loss: 0.916259 | lr 9.3397e-05 | norm: 1.0728 | dt: 1681.25ms | tok/sec: 2436.28\n",
            "step  429 | loss: 1.009572 | lr 9.2495e-05 | norm: 1.1147 | dt: 1681.96ms | tok/sec: 2435.25\n",
            "step  430 | loss: 1.043031 | lr 9.1604e-05 | norm: 1.1730 | dt: 1684.23ms | tok/sec: 2431.97\n",
            "step  431 | loss: 0.989644 | lr 9.0725e-05 | norm: 1.3063 | dt: 1690.09ms | tok/sec: 2423.54\n",
            "step  432 | loss: 0.980034 | lr 8.9858e-05 | norm: 1.2334 | dt: 1679.26ms | tok/sec: 2439.17\n",
            "step  433 | loss: 0.924372 | lr 8.9002e-05 | norm: 1.2427 | dt: 1678.21ms | tok/sec: 2440.70\n",
            "step  434 | loss: 1.025491 | lr 8.8158e-05 | norm: 1.2027 | dt: 1674.33ms | tok/sec: 2446.35\n",
            "step  435 | loss: 1.067528 | lr 8.7326e-05 | norm: 1.2107 | dt: 1677.31ms | tok/sec: 2442.00\n",
            "step  436 | loss: 0.941406 | lr 8.6505e-05 | norm: 1.1447 | dt: 1683.89ms | tok/sec: 2432.47\n",
            "step  437 | loss: 0.952526 | lr 8.5697e-05 | norm: 1.2703 | dt: 1677.18ms | tok/sec: 2442.20\n",
            "step  438 | loss: 0.882192 | lr 8.4900e-05 | norm: 1.2813 | dt: 1682.41ms | tok/sec: 2434.60\n",
            "step  439 | loss: 0.888519 | lr 8.4115e-05 | norm: 1.0494 | dt: 1682.16ms | tok/sec: 2434.97\n",
            "step  440 | loss: 0.922111 | lr 8.3343e-05 | norm: 1.1554 | dt: 1675.44ms | tok/sec: 2444.73\n",
            "step  441 | loss: 0.891808 | lr 8.2582e-05 | norm: 1.2353 | dt: 1684.89ms | tok/sec: 2431.02\n",
            "step  442 | loss: 0.803617 | lr 8.1833e-05 | norm: 0.9972 | dt: 1680.67ms | tok/sec: 2437.12\n",
            "step  443 | loss: 0.995844 | lr 8.1097e-05 | norm: 1.2808 | dt: 1683.04ms | tok/sec: 2433.69\n",
            "step  444 | loss: 0.911588 | lr 8.0373e-05 | norm: 1.2792 | dt: 1690.30ms | tok/sec: 2423.24\n",
            "step  445 | loss: 0.853635 | lr 7.9660e-05 | norm: 1.1656 | dt: 1682.01ms | tok/sec: 2435.18\n",
            "step  446 | loss: 0.851885 | lr 7.8960e-05 | norm: 0.9914 | dt: 1687.23ms | tok/sec: 2427.65\n",
            "step  447 | loss: 0.884402 | lr 7.8273e-05 | norm: 1.2058 | dt: 1684.70ms | tok/sec: 2431.29\n",
            "step  448 | loss: 0.957741 | lr 7.7597e-05 | norm: 1.1437 | dt: 1683.54ms | tok/sec: 2432.97\n",
            "step  449 | loss: 0.747405 | lr 7.6934e-05 | norm: 0.9875 | dt: 1685.80ms | tok/sec: 2429.70\n",
            "step  450 | loss: 0.893224 | lr 7.6283e-05 | norm: 1.1801 | dt: 1685.65ms | tok/sec: 2429.93\n",
            "step  451 | loss: 0.877834 | lr 7.5644e-05 | norm: 1.2271 | dt: 1681.14ms | tok/sec: 2436.44\n",
            "step  452 | loss: 0.936700 | lr 7.5018e-05 | norm: 1.1983 | dt: 1684.81ms | tok/sec: 2431.13\n",
            "step  453 | loss: 0.821879 | lr 7.4405e-05 | norm: 1.0530 | dt: 1682.88ms | tok/sec: 2433.93\n",
            "step  454 | loss: 0.836618 | lr 7.3803e-05 | norm: 1.1316 | dt: 1674.63ms | tok/sec: 2445.92\n",
            "step  455 | loss: 0.823740 | lr 7.3215e-05 | norm: 1.1893 | dt: 1685.57ms | tok/sec: 2430.04\n",
            "step  456 | loss: 0.864017 | lr 7.2639e-05 | norm: 1.2033 | dt: 1687.27ms | tok/sec: 2427.60\n",
            "step  457 | loss: 0.781390 | lr 7.2075e-05 | norm: 1.0497 | dt: 1677.53ms | tok/sec: 2441.68\n",
            "step  458 | loss: 0.781507 | lr 7.1524e-05 | norm: 1.0334 | dt: 1681.29ms | tok/sec: 2436.22\n",
            "step  459 | loss: 0.794267 | lr 7.0985e-05 | norm: 1.0781 | dt: 1680.56ms | tok/sec: 2437.28\n",
            "step  460 | loss: 1.034901 | lr 7.0459e-05 | norm: 1.4324 | dt: 1684.48ms | tok/sec: 2431.62\n",
            "step  461 | loss: 0.911705 | lr 6.9946e-05 | norm: 1.2828 | dt: 1678.46ms | tok/sec: 2440.33\n",
            "step  462 | loss: 0.856152 | lr 6.9446e-05 | norm: 1.0647 | dt: 1682.83ms | tok/sec: 2433.99\n",
            "step  463 | loss: 0.792284 | lr 6.8958e-05 | norm: 1.1004 | dt: 1682.90ms | tok/sec: 2433.90\n",
            "step  464 | loss: 0.754217 | lr 6.8483e-05 | norm: 0.9823 | dt: 1676.47ms | tok/sec: 2443.24\n",
            "step  465 | loss: 0.798002 | lr 6.8020e-05 | norm: 1.0687 | dt: 1680.18ms | tok/sec: 2437.83\n",
            "step  466 | loss: 1.011220 | lr 6.7571e-05 | norm: 1.3132 | dt: 1683.60ms | tok/sec: 2432.88\n",
            "step  467 | loss: 0.795897 | lr 6.7134e-05 | norm: 1.2832 | dt: 1679.97ms | tok/sec: 2438.14\n",
            "step  468 | loss: 0.743724 | lr 6.6710e-05 | norm: 1.0775 | dt: 1680.66ms | tok/sec: 2437.14\n",
            "step  469 | loss: 0.796772 | lr 6.6298e-05 | norm: 1.1060 | dt: 1679.89ms | tok/sec: 2438.26\n",
            "step  470 | loss: 0.838423 | lr 6.5900e-05 | norm: 1.2450 | dt: 1684.52ms | tok/sec: 2431.56\n",
            "step  471 | loss: 0.849503 | lr 6.5515e-05 | norm: 1.2999 | dt: 1676.71ms | tok/sec: 2442.88\n",
            "step  472 | loss: 0.790640 | lr 6.5142e-05 | norm: 1.1531 | dt: 1684.74ms | tok/sec: 2431.23\n",
            "step  473 | loss: 0.886716 | lr 6.4782e-05 | norm: 1.2124 | dt: 1677.54ms | tok/sec: 2441.67\n",
            "step  474 | loss: 0.848728 | lr 6.4436e-05 | norm: 1.2079 | dt: 1681.13ms | tok/sec: 2436.45\n",
            "step  475 | loss: 0.828568 | lr 6.4102e-05 | norm: 1.3590 | dt: 1675.22ms | tok/sec: 2445.05\n",
            "step  476 | loss: 0.842556 | lr 6.3781e-05 | norm: 1.3096 | dt: 1674.06ms | tok/sec: 2446.74\n",
            "step  477 | loss: 0.756590 | lr 6.3473e-05 | norm: 1.4346 | dt: 1679.18ms | tok/sec: 2439.29\n",
            "step  478 | loss: 0.901444 | lr 6.3178e-05 | norm: 1.1754 | dt: 1687.11ms | tok/sec: 2427.82\n",
            "step  479 | loss: 0.730553 | lr 6.2896e-05 | norm: 1.0592 | dt: 1676.47ms | tok/sec: 2443.23\n",
            "step  480 | loss: 0.826497 | lr 6.2628e-05 | norm: 1.0869 | dt: 1676.92ms | tok/sec: 2442.58\n",
            "step  481 | loss: 0.790359 | lr 6.2372e-05 | norm: 1.1573 | dt: 1684.12ms | tok/sec: 2432.13\n",
            "step  482 | loss: 0.734748 | lr 6.2129e-05 | norm: 1.1352 | dt: 1689.32ms | tok/sec: 2424.65\n",
            "step  483 | loss: 0.916569 | lr 6.1899e-05 | norm: 1.4032 | dt: 1681.30ms | tok/sec: 2436.21\n",
            "step  484 | loss: 0.834642 | lr 6.1683e-05 | norm: 1.2572 | dt: 1677.01ms | tok/sec: 2442.44\n",
            "step  485 | loss: 0.763753 | lr 6.1479e-05 | norm: 1.1688 | dt: 1678.41ms | tok/sec: 2440.40\n",
            "step  486 | loss: 0.719163 | lr 6.1289e-05 | norm: 1.2097 | dt: 1680.03ms | tok/sec: 2438.05\n",
            "step  487 | loss: 0.735994 | lr 6.1111e-05 | norm: 1.0958 | dt: 1681.09ms | tok/sec: 2436.52\n",
            "step  488 | loss: 0.722222 | lr 6.0947e-05 | norm: 1.3601 | dt: 1682.72ms | tok/sec: 2434.16\n",
            "step  489 | loss: 0.814659 | lr 6.0796e-05 | norm: 1.2166 | dt: 1681.88ms | tok/sec: 2435.37\n",
            "step  490 | loss: 0.821037 | lr 6.0658e-05 | norm: 1.1691 | dt: 1680.89ms | tok/sec: 2436.80\n",
            "step  491 | loss: 0.746809 | lr 6.0533e-05 | norm: 1.0749 | dt: 1684.79ms | tok/sec: 2431.16\n",
            "step  492 | loss: 0.789640 | lr 6.0421e-05 | norm: 1.1974 | dt: 1683.50ms | tok/sec: 2433.03\n",
            "step  493 | loss: 0.747596 | lr 6.0322e-05 | norm: 1.2837 | dt: 1677.89ms | tok/sec: 2441.16\n",
            "step  494 | loss: 0.724750 | lr 6.0237e-05 | norm: 1.1106 | dt: 1681.44ms | tok/sec: 2436.01\n",
            "step  495 | loss: 0.812240 | lr 6.0164e-05 | norm: 1.1673 | dt: 1676.31ms | tok/sec: 2443.47\n",
            "step  496 | loss: 0.798656 | lr 6.0105e-05 | norm: 1.2170 | dt: 1682.10ms | tok/sec: 2435.05\n",
            "step  497 | loss: 0.852305 | lr 6.0059e-05 | norm: 1.3016 | dt: 1680.89ms | tok/sec: 2436.80\n",
            "step  498 | loss: 0.742622 | lr 6.0026e-05 | norm: 1.3376 | dt: 1680.72ms | tok/sec: 2437.04\n",
            "step  499 | loss: 0.704724 | lr 6.0007e-05 | norm: 1.0973 | dt: 1681.29ms | tok/sec: 2436.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GOMWNbwKZrBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def calculate_perplexity(model, data_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(data_loader, desc=\"Calculating perplexity\"):\n",
        "            # Move the batch to the appropriate device (CPU/GPU)\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass: Get logits and loss from the model\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "            # Accumulate the loss\n",
        "            total_loss += loss.item() * x.size(0) * x.size(1)  # Multiply by batch size and sequence length\n",
        "            total_tokens += x.numel()  # Count the total number of tokens\n",
        "\n",
        "    # Ensure total_tokens is not zero to avoid division by zero\n",
        "    if total_tokens == 0:\n",
        "        print(\"No tokens found, perplexity cannot be computed.\")\n",
        "        return float('inf')\n",
        "\n",
        "    # Calculate average loss (cross-entropy loss per token)\n",
        "    average_loss = total_loss / total_tokens\n",
        "\n",
        "    # Calculate perplexity: perplexity = exp(average loss)\n",
        "    perplexity = torch.exp(torch.tensor(average_loss, device=device))\n",
        "\n",
        "    # Print final results\n",
        "    print(f\"Total Loss: {total_loss:.4f}, Total Tokens: {total_tokens}, Average Loss: {average_loss:.4f}\")\n",
        "    return perplexity.item()\n",
        "\n",
        "# Example usage:\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create dataset for test data\n",
        "test_dataset = ImprovedDataset(file_path='test.txt', config=config, train_mode=False)\n",
        "\n",
        "# Create DataLoader\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=config.num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Assuming you have a trained model loaded on the correct device\n",
        "perplexity = calculate_perplexity(model, test_loader, device)\n",
        "print(f\"Perplexity of the model on test data: {perplexity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ExXic09wSZ8",
        "outputId": "fbf5e857-cbf9-4fa1-b9af-5086fd15898d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from cache...\n",
            "Loaded 8192 tokens from test.txt\n",
            "1 epoch = 2 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating perplexity: 100%|██████████| 1792/1792 [13:09<00:00,  2.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Loss: 44259714.0391, Total Tokens: 7340032, Average Loss: 6.0299\n",
            "Perplexity of the model on test data: 415.6767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eg53CtREwSca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # -----------------------------------------------------------------------------\n",
        "# num_return_sequences = 5\n",
        "# max_length = 30\n",
        "\n",
        "# # model = GPT.from_pretrained('gpt2')\n",
        "# # model.eval()\n",
        "# # model.to('cuda')\n",
        "\n",
        "# # prefix tokens\n",
        "# enc = tiktoken.get_encoding('gpt2')\n",
        "# tokens = enc.encode(\"Hello,\")\n",
        "# tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
        "# tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
        "# x = tokens.to('cuda')\n",
        "\n",
        "# # generate! right now x is (B, T) where B = 5, T = 8\n",
        "# # set the seed to 42\n",
        "# torch.manual_seed(42)\n",
        "# torch.cuda.manual_seed(42)\n",
        "# while x.size(1) < max_length:\n",
        "#     # forward the model to get the logits\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(x) # (B, T, vocab_size)\n",
        "#         # take the logits at the last position\n",
        "#         logits = logits[:, -1, :] # (B, vocab_size)\n",
        "#         # get the probabilities\n",
        "#         probs = F.softmax(logits, dim=-1)\n",
        "#         # do top-k sampling of 50 (huggingface pipeline default)\n",
        "#         # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "#         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "#         # select a token from the top-k probabilities\n",
        "#         # note: multinomial does not demand the input to sum to 1\n",
        "#         ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "#         # gather the corresponding indices\n",
        "#         xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "#         # append to the sequence\n",
        "#         x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# # print the generated text\n",
        "# for i in range(num_return_sequences):\n",
        "#     tokens = x[i, :max_length].tolist()\n",
        "#     decoded = enc.decode(tokens)\n",
        "#     print(\">\", decoded)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Parameters\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "model.to('cuda')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Prefix tokens\n",
        "tokens = tokenizer.encode(\"Hello,\", return_tensors='pt')  # (1, T)\n",
        "tokens = tokens.repeat(num_return_sequences, 1)  # (B, T) where B = num_return_sequences\n",
        "x = tokens.to('cuda')  # Move tokens to GPU\n",
        "\n",
        "# Set the seed\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Generate text\n",
        "while x.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)  # `outputs` is a tuple\n",
        "        logits = outputs.logits  # Extract logits (B, T, vocab_size)\n",
        "        logits = logits[:, -1, :]  # Take logits at the last position (B, vocab_size)\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)  # Top-k sampling\n",
        "\n",
        "        ix = torch.multinomial(topk_probs, 1)  # Sample from top-k probabilities (B, 1)\n",
        "        xcol = torch.gather(topk_indices, -1, ix)  # Gather indices of sampled tokens (B, 1)\n",
        "        x = torch.cat((x, xcol), dim=1)  # Append sampled tokens to the sequence (B, T+1)\n",
        "\n",
        "# Print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :].tolist()\n",
        "    decoded = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "id": "NFaJFeJylzXh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533,
          "referenced_widgets": [
            "501aab92d9854518b91d90e545bd22da",
            "a72e247fbd9b4e85804eb35298e39196",
            "503dcecf04c1402e9ca251d30fa0baf6",
            "29d8169cfa434e73bf33b8b4afa5d002",
            "a3859254c0f64bf8a0d60fe679150b44",
            "c4a19eb0023d4c09aeb084299aa8d113",
            "ab99d7725cd043d19ba614f7ad041d28",
            "fb5f922b8fac43a097b551f22173fe49",
            "cfe2504c193e48d0bf109912f953391c",
            "58cb979b55c1416eb575fef2f7071993",
            "4d809795b4ce4e639e43a2823f876eb0",
            "a06a9df9dddc43bb9cae2edd3ceb07d4",
            "8603bd7cecab48f19f690b863989c0e8",
            "89d7952eb4ec4d6d91fa5817c2e30bff",
            "e715c946ec934aab91b4c1ddc1c184ea",
            "9ef47759bf1543b8ad2022530f51dfca",
            "ab924ff7a7c443098e339102a38f4b3a",
            "08df044499db4447bebd087f88792877",
            "990a592654cd4b27a78fa0c9d2ac2f21",
            "5d8e518e33cb46b1ae5cec896ddd4e37",
            "244d7ac25cb34f8dad3d1040c74c8e15",
            "77963fe50e4a4f5990933084562e1e33",
            "0aa5f9dee04e49f1bd3d23a676ffc526",
            "9e2f344d5e354b649710ea3757fa34e7",
            "d5902f6743aa45e4b7ade5828d72a730",
            "f93fa08962034d50b3a461541246ef37",
            "b6379966fb6d4601b875d8d455bde207",
            "01d3909a50804db49b1b4fd38ef00be4",
            "0a2d4c76fa0243939c00890faf6a30fc",
            "20aaa8d239a046f1a57a816ecefcbe9b",
            "6c88c16c634b4fb1a6622ce2a00cdeb7",
            "e935829e40c9433a85bb7b95819dc153",
            "ce6a3e8b77334b308a2cbc141f9ceb8c",
            "d69c5153b727402a8778dc9ee8bdb2be",
            "969e019ca9aa4277b0fa101aad48eed8",
            "cb6821b60e9447febd30782c47c42d3b",
            "5fff2c099783449788da541ac4248c70",
            "bbb437e3fc9d4a9ea62d8e721614ae33",
            "facf33d518a84ba2bbaec714d256695f",
            "4fcb2ea2a28f46c5a5c3df8480423946",
            "61a12921971044eca41155925601dd67",
            "bd75dbda7ee6440c81a74d08ec46a3da",
            "08b8086093214864bd0102a630753be6",
            "6cd4444ac9cd4c1784cb4e44fca5755f",
            "289521e212274846b7090cc987a4b532",
            "6fa6ef85c9924323b153194b28ec1a3d",
            "804ce020cb544f06964850503a382f20",
            "d0308687e8a04d41af1a90bbdb8fa5e4",
            "bddf19afc27c4e978c2885980e9da94f",
            "10e0d9af798647a489a914adc3b57cb6",
            "52125e5963274802b22a46d1d5186c45",
            "f9186aef158c42aa9d6a9ae2c22710ef",
            "ecece94338114780aa18f59f13d8b9d9",
            "83b3db635b9a4a8699608526a1dc0907",
            "82c88d1aed2b4e4fb52465b9ee10dac4",
            "407185bbace640cda96354f0c47b95e6",
            "55854e8aaa484c938901ba61f39a3e11",
            "7b2a67a796d4461fbe36f8fa74fd33e5",
            "49e2989c338c47438bc568997aaa7524",
            "40ea4a03c14f4513b61eb086db97dccd",
            "2cf832e676934892b30a62b58d51618f",
            "6e1b007c822a4835bd3f4035d7e36a92",
            "4291a453f88a411db15f07b87a8139ae",
            "79681163f52244b6b76bbc5859f2af59",
            "3a8e6a7cd8004721b9881321692c6b7b",
            "fabb336945b6459abbea0b31bcaed5ae",
            "a92a0366886e4d4594c65b4637fd2ef6",
            "13bf67cda8c54570a0c49fa06764717b",
            "92533245f07045179e10255e16dd6a1b",
            "45bf6152ee5d49e3b136a1b99f4d9d00",
            "91e3bf7688e7406980a16c579195fc63",
            "2d1d5a31b66b4f0da8e70ac13669ebaa",
            "50ee91a735ad42e2b1c6dc6280d6cf00",
            "bc3e4dbdaab74cb18c6da8b46b9d1161",
            "3635057b0f93432fbbe23d2f4fc9db4d",
            "d5f53b7e42674dcd9d5007ddcb76e1f1",
            "1651fe5bf73c45a8a481db24ced2cde2"
          ]
        },
        "outputId": "21dedea4-8053-423d-a3b2-7dde99e6ef8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "501aab92d9854518b91d90e545bd22da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a06a9df9dddc43bb9cae2edd3ceb07d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0aa5f9dee04e49f1bd3d23a676ffc526"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d69c5153b727402a8778dc9ee8bdb2be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "289521e212274846b7090cc987a4b532"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "407185bbace640cda96354f0c47b95e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a92a0366886e4d4594c65b4637fd2ef6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, now that is a great start.\"\n",
            "\n",
            "Houffrani noted his own recent victories over other world champions like Chris Froome and\n",
            "> Hello, why you like this, I am from the UK here… I don't think you are ready to see it. But I'm glad to\n",
            "> Hello, this is me and my mother. You know we are all the same. We are all the same. We all believe in each other.\n",
            "> Hello, we would like to thank you all for supporting H1Z1 today. It will be extremely important to you, everyone, that we continue\n",
            "> Hello,\n",
            "\n",
            "We know that some people get bored from it. You know, people think they can do it. This is good. Well,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import tiktoken\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('punkt')\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK data: {e}\")\n",
        "\n",
        "class TextGenerator:\n",
        "    def __init__(self, model_name='gpt2', device='cuda'):\n",
        "        self.device = device\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "        self.model.to(device)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def generate_text(self, prompt, num_return_sequences=5, max_length=50):\n",
        "        # Encode and prepare input\n",
        "        tokens = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "        x = tokens.repeat(num_return_sequences, 1).to(self.device)\n",
        "\n",
        "        # Generate text\n",
        "        while x.size(1) < max_length:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(x)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "                # Top-k sampling\n",
        "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "                ix = torch.multinomial(topk_probs, 1)\n",
        "                xcol = torch.gather(topk_indices, -1, ix)\n",
        "                x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "        # Decode generated sequences\n",
        "        generated_texts = []\n",
        "        for i in range(num_return_sequences):\n",
        "            tokens = x[i, :].tolist()\n",
        "            decoded = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "            generated_texts.append(decoded)\n",
        "\n",
        "        return generated_texts\n",
        "\n",
        "def calculate_word_level_f1(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate word-level F1 score between reference and candidate texts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Tokenize both texts into words\n",
        "        ref_words = set(word_tokenize(reference.lower()))\n",
        "        cand_words = set(word_tokenize(candidate.lower()))\n",
        "    except Exception as e:\n",
        "        print(f\"Tokenization error: {e}\")\n",
        "        # Fallback to simple space-based tokenization\n",
        "        ref_words = set(reference.lower().split())\n",
        "        cand_words = set(candidate.lower().split())\n",
        "\n",
        "    # Calculate intersection\n",
        "    common_words = ref_words.intersection(cand_words)\n",
        "\n",
        "    # Calculate precision, recall, and F1\n",
        "    precision = len(common_words) / len(cand_words) if cand_words else 0\n",
        "    recall = len(common_words) / len(ref_words) if ref_words else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "def evaluate_single_generation(reference, generated_text):\n",
        "    \"\"\"\n",
        "    Evaluate a single generated text against its reference.\n",
        "    \"\"\"\n",
        "    precision, recall, f1 = calculate_word_level_f1(reference, generated_text)\n",
        "\n",
        "    # Calculate BLEU score for single text\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_score = corpus_bleu([[reference.split()]], [generated_text.split()], smoothing_function=smoothie)\n",
        "\n",
        "    return {\n",
        "        'bleu': bleu_score,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "def evaluate_generations(generator, test_loader=None, num_samples=100):\n",
        "    \"\"\"\n",
        "    Evaluate generated text using both BLEU and F1 scores.\n",
        "    \"\"\"\n",
        "    if test_loader is None:\n",
        "        # If no test_loader provided, evaluate the sample generations\n",
        "        sample_texts = generator.generate_text(\"Hello,\", num_return_sequences=5, max_length=30)\n",
        "        # Use first generation as reference (for demonstration)\n",
        "        reference = sample_texts[0]\n",
        "        scores_list = []\n",
        "\n",
        "        for idx, text in enumerate(sample_texts):\n",
        "            print(f\"Generated text {idx + 1}: {text}\")\n",
        "            if idx > 0:  # Skip comparing first text with itself\n",
        "                scores = evaluate_single_generation(reference, text)\n",
        "                scores_list.append(scores)\n",
        "\n",
        "        # Average scores\n",
        "        avg_scores = {\n",
        "            'bleu': np.mean([s['bleu'] for s in scores_list]),\n",
        "            'f1': np.mean([s['f1'] for s in scores_list]),\n",
        "            'precision': np.mean([s['precision'] for s in scores_list]),\n",
        "            'recall': np.mean([s['recall'] for s in scores_list])\n",
        "        }\n",
        "        return avg_scores\n",
        "\n",
        "    else:\n",
        "        # Original evaluation with test_loader\n",
        "        references = []\n",
        "        candidates = []\n",
        "        f1_scores = []\n",
        "        precisions = []\n",
        "        recalls = []\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "        for idx, (x, y) in enumerate(test_loader):\n",
        "            if idx >= num_samples:\n",
        "                break\n",
        "\n",
        "            prompt = enc.decode(x[0].tolist()[:10])\n",
        "            generated = generator.generate_text(prompt, num_return_sequences=1)[0]\n",
        "            reference = enc.decode(y[0].tolist()[:50])\n",
        "\n",
        "            precision, recall, f1 = calculate_word_level_f1(reference, generated)\n",
        "            f1_scores.append(f1)\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "\n",
        "            references.append([reference.split()])\n",
        "            candidates.append(generated.split())\n",
        "\n",
        "        bleu_score = corpus_bleu(references, candidates, smoothing_function=SmoothingFunction().method4)\n",
        "\n",
        "        return {\n",
        "            'bleu': bleu_score,\n",
        "            'f1': np.mean(f1_scores),\n",
        "            'precision': np.mean(precisions),\n",
        "            'recall': np.mean(recalls)\n",
        "        }\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "    # Initialize generator\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    generator = TextGenerator(device=device)\n",
        "\n",
        "    # Generate and evaluate sample texts\n",
        "    scores = evaluate_generations(generator)  # No test_loader needed\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"BLEU Score: {scores['bleu']:.4f}\")\n",
        "    print(f\"F1 Score: {scores['f1']:.4f}\")\n",
        "    print(f\"Precision: {scores['precision']:.4f}\")\n",
        "    print(f\"Recall: {scores['recall']:.4f}\")"
      ],
      "metadata": {
        "id": "3rpANHpqwShD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997633c7-cdf3-4209-86d0-9d542ee14e6c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text 1: Hello, now that is a great start.\"\n",
            "\n",
            "Houffrani noted his own recent victories over other world champions like Chris Froome and\n",
            "Generated text 2: Hello, why you like this, I am from the UK here… I don't think you are ready to see it. But I'm glad to\n",
            "Tokenization error: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Generated text 3: Hello, this is me and my mother. You know we are all the same. We are all the same. We all believe in each other.\n",
            "Tokenization error: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Generated text 4: Hello, we would like to thank you all for supporting H1Z1 today. It will be extremely important to you, everyone, that we continue\n",
            "Tokenization error: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Generated text 5: Hello,\n",
            "\n",
            "We know that some people get bored from it. You know, people think they can do it. This is good. Well,\n",
            "Tokenization error: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "\n",
            "Evaluation Results:\n",
            "BLEU Score: 0.0147\n",
            "F1 Score: 0.1346\n",
            "Precision: 0.1387\n",
            "Recall: 0.1310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nltk\n",
        "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def evaluate_distilbert():\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
        "    model.eval()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "\n",
        "    def generate_text(prompt, max_length=50, temperature=0.7):\n",
        "        current_text = prompt\n",
        "        words = current_text.split()\n",
        "\n",
        "        # Set of tokens to filter out\n",
        "        filter_tokens = {'[PAD]', '[SEP]', '[CLS]', '.', ',', '!', '?', '\"', \"'\"}\n",
        "\n",
        "        for _ in range(max_length - len(words)):\n",
        "            input_text = current_text + \" \" + tokenizer.mask_token\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            mask_idx = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "            if len(mask_idx) == 0:  # No mask token found\n",
        "                break\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits[0, mask_idx[0]]\n",
        "\n",
        "                # Apply temperature\n",
        "                logits = logits / temperature\n",
        "\n",
        "                # Apply softmax to get probabilities\n",
        "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "                # Sample from top-k\n",
        "                top_k = 40\n",
        "                top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
        "                selected_idx = torch.multinomial(top_k_probs, 1)\n",
        "                predicted_token_id = top_k_indices[selected_idx].item()\n",
        "\n",
        "            predicted_word = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "            # Skip if predicted word is in filter set\n",
        "            if predicted_word.strip() in filter_tokens:\n",
        "                continue\n",
        "\n",
        "            # Add word only if it's not repetitive\n",
        "            words = current_text.split()\n",
        "            if len(words) >= 3 and predicted_word.strip() == words[-1]:\n",
        "                continue\n",
        "\n",
        "            current_text += \" \" + predicted_word.strip()\n",
        "\n",
        "            # Add occasional punctuation\n",
        "            if len(current_text.split()) % 8 == 0:\n",
        "                current_text += \".\"\n",
        "\n",
        "            # Stop if text gets too long\n",
        "            if len(current_text.split()) >= max_length:\n",
        "                break\n",
        "\n",
        "        # Clean up the text\n",
        "        current_text = current_text.replace(\" .\", \".\").replace(\" ,\", \",\")\n",
        "        current_text = \" \".join(current_text.split())  # Remove extra spaces\n",
        "        if not current_text.endswith((\".\", \"!\", \"?\")):\n",
        "            current_text += \".\"\n",
        "\n",
        "        return current_text\n",
        "\n",
        "    # Generate sample texts\n",
        "    prompts = [\n",
        "        \"Hello, my name is\",\n",
        "        \"The weather today looks\",\n",
        "        \"I think this is\",\n",
        "        \"Today is going to be\",\n",
        "        \"The best thing about\"\n",
        "    ]\n",
        "\n",
        "    generated_texts = []\n",
        "\n",
        "    print(\"\\nGenerated Texts:\")\n",
        "    print(\"---------------\")\n",
        "    for prompt in prompts:\n",
        "        generated = generate_text(prompt, max_length=20)  # Shorter length for more focused generation\n",
        "        generated_texts.append(generated)\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    reference = generated_texts[0]\n",
        "    scores_list = []\n",
        "\n",
        "    for text in generated_texts[1:]:\n",
        "        # Calculate F1\n",
        "        ref_words = set(reference.lower().split())\n",
        "        gen_words = set(text.lower().split())\n",
        "        common_words = ref_words.intersection(gen_words)\n",
        "\n",
        "        precision = len(common_words) / len(gen_words) if gen_words else 0\n",
        "        recall = len(common_words) / len(ref_words) if ref_words else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        # Calculate BLEU\n",
        "        smoothie = SmoothingFunction().method4\n",
        "        bleu = corpus_bleu([[reference.split()]], [text.split()], smoothing_function=smoothie)\n",
        "\n",
        "        scores_list.append({\n",
        "            'bleu': bleu,\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        })\n",
        "\n",
        "    # Average scores\n",
        "    avg_scores = {\n",
        "        'bleu': np.mean([s['bleu'] for s in scores_list]),\n",
        "        'f1': np.mean([s['f1'] for s in scores_list]),\n",
        "        'precision': np.mean([s['precision'] for s in scores_list]),\n",
        "        'recall': np.mean([s['recall'] for s in scores_list])\n",
        "    }\n",
        "\n",
        "    print(\"\\nDistilBERT Evaluation Results:\")\n",
        "    print(\"----------------------------\")\n",
        "    print(f\"BLEU Score: {avg_scores['bleu']:.4f}\")\n",
        "    print(f\"F1 Score: {avg_scores['f1']:.4f}\")\n",
        "    print(f\"Precision: {avg_scores['precision']:.4f}\")\n",
        "    print(f\"Recall: {avg_scores['recall']:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Evaluating DistilBERT...\")\n",
        "    evaluate_distilbert()"
      ],
      "metadata": {
        "id": "Zx396KiOtlQJ",
        "outputId": "035575b3-552c-445e-b315-9fe1e53950fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating DistilBERT...\n",
            "\n",
            "Generated Texts:\n",
            "---------------\n",
            "\n",
            "Prompt: Hello, my name is\n",
            "Generated: Hello, my name is susan.\n",
            "\n",
            "Prompt: The weather today looks\n",
            "Generated: The weather today looks incredible.\n",
            "\n",
            "Prompt: I think this is\n",
            "Generated: I think this is silly.\n",
            "\n",
            "Prompt: Today is going to be\n",
            "Generated: Today is going to be great.\n",
            "\n",
            "Prompt: The best thing about\n",
            "Generated: The best thing about him : ;.\n",
            "\n",
            "DistilBERT Evaluation Results:\n",
            "----------------------------\n",
            "BLEU Score: 0.0208\n",
            "F1 Score: 0.0955\n",
            "Precision: 0.0917\n",
            "Recall: 0.1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybBsVCDntlMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cJsNSRxCtlKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TU35IkjotlIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DSJgxLG1tlGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}